[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "AI is everywhere!\nSome notable examples:\n\ngoogle lens (region proposal network)\ngoogle translate (Transformer-based)\nYouTube automatic captioning (automatic speech recognition)\nGmail spam filters (rule-based filters + Density clustering)\napple’s faceID (deep convolutional networks)\nTesla autonomous car (deep learning)\nvirtual assistant (Siri, Google Assistant)\nNVIDIA DLSS (Deep learning supersampling)\nBloomberg (NLP sentiment analysis)\n\n\n\n\n\n\nFigure 1: General goal of AI1\n\n\n\n\n\n\nData science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.\n\n\n\n\n\n\n\n\nFigure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows.\n\n\n\n\n\n\n\n\nFigure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated!\n\n\n\n\n\n\n\n\nFigure 7: Data and computational trend\n\n\n\n\nIn modern days, it is well established fact that the modern AI/ML systems success has been critically dependent on their ability to process massive amounts of raw data in a parallel fashion using task-optimized hardware.\n\n\n\n\n\nFigure 8: Data Processing Evolution7\n\n\n\n\n\n\n\n\n\nFigure 9: NVIDIA GPU FP32 Cores evolution8"
  },
  {
    "objectID": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "href": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "title": "Overview",
    "section": "",
    "text": "Data science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data."
  },
  {
    "objectID": "index.html#data-scientist-vs-machine-learning-engineer",
    "href": "index.html#data-scientist-vs-machine-learning-engineer",
    "title": "Overview",
    "section": "",
    "text": "Figure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows."
  },
  {
    "objectID": "index.html#reality-ml-in-production",
    "href": "index.html#reality-ml-in-production",
    "title": "Overview",
    "section": "",
    "text": "Figure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated!"
  },
  {
    "objectID": "index.html#reality-big-data-fast-compute",
    "href": "index.html#reality-big-data-fast-compute",
    "title": "Overview",
    "section": "",
    "text": "Figure 7: Data and computational trend\n\n\n\n\nIn modern days, it is well established fact that the modern AI/ML systems success has been critically dependent on their ability to process massive amounts of raw data in a parallel fashion using task-optimized hardware.\n\n\n\n\n\nFigure 8: Data Processing Evolution7\n\n\n\n\n\n\n\n\n\nFigure 9: NVIDIA GPU FP32 Cores evolution8"
  },
  {
    "objectID": "content/learning_2.html",
    "href": "content/learning_2.html",
    "title": "About Learning II",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).\n\n\n\nThe objective of “learning” is to simultaneously:\n\nAchieve LOW variance of Estimator\nAchieve LOW Bias of Estimator\n\n\n\n\n\n\nFigure 1: High Variance and High Bias\n\n\nWe have a poor estimator. Poor fit and poor predictor for training sample as well as for test sample.\n\n\n\n\n\n\nFigure 2: High Variance and Low Bias\n\n\nWe have a low “precision” predictor. In another word, we have an over-fitting, and hence the precision is poor.\nOverfitting: too much reliance on the training data\n\n\n\n\n\n\nFigure 3: Low Variance and High Bias\n\n\nWe have precise predictor, but will work well only for training sample, however will be problematic when applied to cases test sample. This is the case of underfitting.\nUnderfitting: a failure to learn the relationships in the data\n\n\n\n\n\n\nFigure 4: Low Variance and Low Bias\n\n\nThe predictor will have a good fit for both training sample and test sample. This is what we want.\n\n\n\n\nBase on our earlier objective:\n\n\n\nFigure 5: Training trade-off\n\n\n\n\n\nVijay Kotu and Bala Deshpande, Data Science Concepts and Practice, 2nd Ed., Elsevier Inc, 2019.\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nScott Fortmann-Roe, Understanding the Bias-Variance Tradeoff, 2012\nMachine Learning for Intelligent Systems: Bias-Variance Tradeoff\nML 101: Bias and Variance\nBias and Variance in Machine Learning"
  },
  {
    "objectID": "content/learning_2.html#the-problem-setting-of-statistical-learning.",
    "href": "content/learning_2.html#the-problem-setting-of-statistical-learning.",
    "title": "About Learning II",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data)."
  },
  {
    "objectID": "content/learning_2.html#error-and-bias",
    "href": "content/learning_2.html#error-and-bias",
    "title": "About Learning II",
    "section": "",
    "text": "The objective of “learning” is to simultaneously:\n\nAchieve LOW variance of Estimator\nAchieve LOW Bias of Estimator\n\n\n\n\n\n\nFigure 1: High Variance and High Bias\n\n\nWe have a poor estimator. Poor fit and poor predictor for training sample as well as for test sample.\n\n\n\n\n\n\nFigure 2: High Variance and Low Bias\n\n\nWe have a low “precision” predictor. In another word, we have an over-fitting, and hence the precision is poor.\nOverfitting: too much reliance on the training data\n\n\n\n\n\n\nFigure 3: Low Variance and High Bias\n\n\nWe have precise predictor, but will work well only for training sample, however will be problematic when applied to cases test sample. This is the case of underfitting.\nUnderfitting: a failure to learn the relationships in the data\n\n\n\n\n\n\nFigure 4: Low Variance and Low Bias\n\n\nThe predictor will have a good fit for both training sample and test sample. This is what we want."
  },
  {
    "objectID": "content/learning_2.html#trade-off",
    "href": "content/learning_2.html#trade-off",
    "title": "About Learning II",
    "section": "",
    "text": "Base on our earlier objective:\n\n\n\nFigure 5: Training trade-off\n\n\n\n\n\nVijay Kotu and Bala Deshpande, Data Science Concepts and Practice, 2nd Ed., Elsevier Inc, 2019.\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nScott Fortmann-Roe, Understanding the Bias-Variance Tradeoff, 2012\nMachine Learning for Intelligent Systems: Bias-Variance Tradeoff\nML 101: Bias and Variance\nBias and Variance in Machine Learning"
  },
  {
    "objectID": "content/intro-data_2.html",
    "href": "content/intro-data_2.html",
    "title": "Data & Sampling",
    "section": "",
    "text": "Data & Sampling\nYou’ve learnt basic about EDA & basic of machine “learning” process. Now, let’s go a bit further."
  },
  {
    "objectID": "content/data_2.html",
    "href": "content/data_2.html",
    "title": "Data and sampling",
    "section": "",
    "text": "First we must differentiate between data at hand - which is the data that is available to us as modeller, and data not in hand, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to “estimator errors” and “estimator bias” that we discuss before.\nSecond, we have to be cognizant of the sample size of the data at hand. We always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes. Size and entropy matters because Law of Large Numbers hypothesis rely heavily on it. If the size is not too large and yet have high enough entropy, the hypothesis is tested with high power (low \\(\\beta\\), Type II error); and yet large size with low entropy will reduce Type I error (Low \\(\\alpha\\), increase True Negative).\nThird, we must take note of problems within the data itself, namely the “true process” of data generations. This is at best is unknown - but is assumed to follow stable process which are Gaussian, ergodic, and stationary in nature. This is far from true in many real applications. If we go back to earlier discussions, this is included under the “pure errors” part of the estimation process. The key assumption that we made is that residual errors (“pure errors”,“white noise”) follows an IID process (Independent and Identically Distributed). This is a strong assumption which to most modellers, they just assumed that it holds all the time. We know from practice that this IID assumption fails and fail miserably at time.\nData transformation\nMany times in ML we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that.\nData augmentation\nData augmentation is used when we want to add “organized entropy” into an existing data set. This is only meaningful if those entropy helps to expand the data at hand, without altering the “meaninful aspects” of the data. That’s why it is useful in image processing or voice, which structures are not altered, but increase the space and dimensions.\nData de-noising\nOpposite to augmentation is “de-noising”, where we apply filters to take out the noises in the data. The argument here is reverse that is to reduce entropy in the data. Again, this is meaningful if there are no alterations to the basic structure of the data; and hence useful in image or voice processing. In NLP, removal of stop-words is a de-noising exercise.\nData pre-processing\nData pre-processing may involve all of the above: transformation, augmentation and de-noising. In some cases all are required and helpful, in some cases a mixture of them will do. The basic process however is always data transformation.\n\n\n\nScaling\n\nScaling and centering (if needed) is the most basic method. Why its needed (beside computing reasons)? To enable interpretations of the model to be easier, because most of the cases we are dealing with probabilities, which is a number in [0,1]. Furthermore, this will standardied numbers of various scales into same unique scale.\n\nLog transformation\n\nLogarithmic function acts both as dampener and smoother. However, log has one major problem: log(0) is undefined. So we must deal with numbers which are strictly positive, such as (0,1]. Log transformation is generally very useful, especially for highly skewed data.\n\nVectorization\n\nVectorization is a method of mapping a set of data onto a smaller space, represented by unique mapping between the raw data and a vector space, which serves as a “look-up table”. Vectorizing does not alter the structure of the data, instead it just compressed the data into a smaller space in terms of computer memories. Instead of working with raw data, we deal with its vectorize representations. An example of this hashing algorithms, which converts any non-fixed size elements onto unique fixed size space. Tokenization is another used method for vectorization in NLP.\n\nBinning and encoding\n\nBinning is used when we want to convert continous variables to categorical variables. There are few methods used:\n\nEqual width binning - what is normally used in general\nFrequence width binning - grouped by largest frequencies instead of equal width\nEntropy based binning - retaining the largest amount of information regarding the ranks of the data by discretizing into a uniform distribution.\n\nEncoding is the opposite of binning, where we convert categorical variables to continous variables. There are a few methods used:\n\nLabel encoding\nBinary encoding\nOrdinal encoding\nOne hot encoding\nFrequency encoding\nTarget mean encoding\n\n\nPower transformer\n\nPower transformation is used to transform non-Gaussian type distribution (such as many outliers or highly skewed) to have more of Gaussian type distribution; there are two popular methods used:\n\nBox-Cox transformation - values must be strictly positive\nYeo-Johnson transformer - values can be negative\n\n\nMissing values imputation\n\nThere are a number of methods:\n\nmean, median, etc., basic statistical method\nk-NN\nMLP\nSelf-organization Map (SOM)\n\n\nDimensionality reduction\n\nThe more complex part of data pre-processing is dimensionality reduction. This is a non-trivial process. Mathematically, this is a “map-reduce” process; where anywhere possible, set of variables are mapped together into a variable which becomes a representative of the set, and this is performed over various possible distinct sets, and the final output will be represented by these representative variables, which will be supplied to the model.\n\nChanging data types\n\nFrom integer to factor, factor to integer, numerical to categorical, ordinal to categorical, etc.\n\nMultilabels hierarchical data\n\nAnother approach is called “Multi hierarchical labels”\n\n\n\nSometimes this is called “Model Tuning” to overcome “over-fitting”.\nThe steps are:\n\nStart with a randomized sample - fit the model - predict on hold out sample\nResample and repeat step 1\nProfile all the sample and resample into a performance profile\nDecide the final tuning parameters\nRefit the model using the entire dataset\n\nManaging data splitting to accomodate for:\n\nPre-processing the predictor data (Xs)\nEstimating model parameters (MSE etc)\nSelect predictors (subset of Xs)\nEvaluate model performance\nFine tuning using ROC curves etc.\n\nResampling techniques:\n\nGeneral cross validation: set the training versus validation set, and finally testing set.\nk-fold cross validation: partition the data to k equal size. Model is fit using all except first subset (first fold), return the first subset to the data and hold out second set, repeat, and so on.\nRepeated CV : apply method 1 but keep randomizing the sets.\nBootstrap : sampling data with replacement by dividing into “in the bag” and “out of bag” samples.\n\n\n\n\n\n\n\nAssociation, that is \\(Pr(Y|X)\\)\nClassification, that is \\(if(x_1 &gt; a)\\) and \\(if(x_2 &lt; b)\\) then ..\nPattern recognition\n\nPixel based (image and video)\nword based (NLP)\nfrequency based (sound, etc.)\n\nKnowledge extraction\n\nemergent structures\nextreme observations (outlier) detection\n\nReinforcement learning\n\nGame theoretic\nProbabilistic learning\n\nNetwork or Graph theory based learning\n\nDriven by network/graph theory\nEmergent\nScaling\n\n\n\n\n\nIt is important to know the objective; this will determine which is more important in choosing models and methods\nClassically in statistics - this is called the “degrees of freedom” of any statistical tests and estimators.\nTrade-offs between interpretability and flexibility is a major issue."
  },
  {
    "objectID": "content/data_2.html#predictive-modelling",
    "href": "content/data_2.html#predictive-modelling",
    "title": "Data and sampling",
    "section": "",
    "text": "Association, that is \\(Pr(Y|X)\\)\nClassification, that is \\(if(x_1 &gt; a)\\) and \\(if(x_2 &lt; b)\\) then ..\nPattern recognition\n\nPixel based (image and video)\nword based (NLP)\nfrequency based (sound, etc.)\n\nKnowledge extraction\n\nemergent structures\nextreme observations (outlier) detection\n\nReinforcement learning\n\nGame theoretic\nProbabilistic learning\n\nNetwork or Graph theory based learning\n\nDriven by network/graph theory\nEmergent\nScaling\n\n\n\n\n\nIt is important to know the objective; this will determine which is more important in choosing models and methods\nClassically in statistics - this is called the “degrees of freedom” of any statistical tests and estimators.\nTrade-offs between interpretability and flexibility is a major issue."
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "About Data",
    "section": "",
    "text": "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform basic data exploration to study the basic characteristics of a dataset.\nBy studying it basic properties, we may find useful patterns, connections, and relationships within data. This is usually called data exploration or exploratory data analysis (EDA).\nThe whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is good or not.\nOnce we have good data, we can training our machine learning model and get accurate results."
  },
  {
    "objectID": "content/data.html#data-barier-to-widespread-ai-adoption",
    "href": "content/data.html#data-barier-to-widespread-ai-adoption",
    "title": "About Data",
    "section": "Data barier to widespread AI adoption",
    "text": "Data barier to widespread AI adoption\n\nsmall datasets\nLong tail problem (customization)1\n\n\n\n\nFigure 1: the need of customization of specific AI projects"
  },
  {
    "objectID": "content/data.html#data-centric-ai",
    "href": "content/data.html#data-centric-ai",
    "title": "About Data",
    "section": "Data-Centric AI",
    "text": "Data-Centric AI\nCurrently, the majority of AI applications are model-centric, one possible reason behind this is that the AI sector pays careful attention to academic research on models. According to Andrew Ng2, more than 90% of research papers in this domain are model-centric. In this approach, data is frequently overlooked, and data collection is viewed as a one-time event.\nIn model-centric approach, you hold the model or code fixed and iteratively improve the quality of the data. This is shift from big data to good data."
  },
  {
    "objectID": "content/data.html#goodness-of-dataset",
    "href": "content/data.html#goodness-of-dataset",
    "title": "About Data",
    "section": "Goodness of dataset:",
    "text": "Goodness of dataset:\nWhat makes a good dataset?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)\n\ndata cleaning is no longer merely a “pre-processing” step, but an iterative ML process!"
  },
  {
    "objectID": "content/data.html#dataset-source-type",
    "href": "content/data.html#dataset-source-type",
    "title": "About Data",
    "section": "Dataset Source Type",
    "text": "Dataset Source Type\nCommon type of dataset is:\n\ncategorical data (image, voice, videos)\ntabular/numerical data\ntime series\ntext"
  },
  {
    "objectID": "content/data.html#categorical-data",
    "href": "content/data.html#categorical-data",
    "title": "About Data",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nExample 1\nImagine you have mnist image data below.\n\n\n\n\n\nFigure 2: sample images of mnist dataset\n\n\n\n\nNow, since our data now is images and not tabular. We can use simple histogram plot like Figure 3 to view each class of MNIST dataset.\n\nWe can observe that data distribution almost the same (almost balanced) for each class\n\n\n\n\n\n\nFigure 3: images distribution of mnist dataset"
  },
  {
    "objectID": "content/data.html#tabular-data",
    "href": "content/data.html#tabular-data",
    "title": "About Data",
    "section": "Tabular Data",
    "text": "Tabular Data\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\nFigure 4: correlation plot\n\n\nSo, in statistical terms we use correlation to denote association between two quantitative variables.\n\nExample 1\nImagine you have tabular data as below.\n\n\n\n\n\n\n\nTable 1: Dummy Dataset\n\n\n\nfeature_A\nfeature_B\nfeature_D\n\n\n\n\n0\n14.54\n32.66\n16\n\n\n1\n28.29\n77.11\n0\n\n\n2\n33.47\n88.45\n3\n\n\n3\n28.73\n93.44\n30\n\n\n4\n24.37\n88.45\n4\n\n\n\n\n\n\n\n\nDataset in Table 1 have 7 columns \\(\\Longrightarrow\\) 7 features\nQuestion : From Figure 4, what is the best way to describe or visualize the data given to us? Answer: Let’s do pair-plot (combination of scatter plot)\n\n\n\n\n\nFigure 5: pair plot\n\n\n\n\nWhat can we say about Figure 5 ?\n\n\n\n\n\nFigure 6: big pair plot\n\n\n\n\n\n\nExample 2\n\n\n\n\n\n\n\nTable 2: Dummy Big Dataset\n\n\n\nfeature_B\nfeature_C\nfeature_D\nfeature_E\nfeature_F\nfeature_G\nfeature_H\nfeature_I\nfeature_J\nfeature_K\n...\nfeature_M\nfeature_M.1\nfeature_O\nfeature_P\nfeature_Q\nfeature_R\nfeature_S\nfeature_T\nfeature_U\nfeature_V\n\n\n\n\n0\n27.0\n30.1\n24.0\n83.68\n0.3\n100.0\nrain\n0\n0\n4.0\n...\n0.0\n1011.0\n26.8\n10.0\n0\nNaN\n0\n10\nRain, Partially cloudy\nrain\n\n\n1\n27.0\n30.0\n24.0\n84.04\n0.0\n0.0\nNaN\n0\n0\n3.2\n...\n0.0\n1011.0\n26.8\n10.0\n0\nNaN\n0\n10\nPartially cloudy\npartly-cloudy-night\n\n\n2\n27.0\n29.6\n23.0\n79.12\n0.0\n0.0\nNaN\n0\n0\n3.2\n...\n0.0\n1010.9\n26.8\n10.0\n0\nNaN\n0\n10\nPartially cloudy\npartly-cloudy-night\n\n\n3\n26.9\n29.9\n24.0\n84.40\n0.0\n0.0\nNaN\n0\n0\n4.7\n...\n0.0\n1010.0\n25.0\n9.9\n0\nNaN\n0\n10\nPartially cloudy\npartly-cloudy-night\n\n\n4\n26.0\n26.0\n24.0\n88.75\n0.9\n100.0\nrain\n0\n0\n5.0\n...\n0.0\n1009.9\n25.0\n9.9\n0\nNaN\n0\n10\nRain, Partially cloudy\nrain\n\n\n\n\n\n5 rows × 21 columns\n\n\n\nBut if we have many features like Table 2 and want to plot pair-plot like Figure 6, seem to overwhelming and confuse isn’t?\nSolution? Use correlation heatmap \\(\\Longrightarrow\\) easier to see based on correlation value/coefficient, r (recall our Figure 4).r value is the degree of association.\n\n\n\n\nTable 3: correlation guide\n\n\nCorrelation Value (r)\nStrength of Relationship\n\n\n\n\nr &lt; 0.3\nNone or very weak\n\n\n0.3 &lt; r &lt;0.5\nWeak\n\n\n0.5 &lt; r &lt; 0.7\nModerate\n\n\nr &gt; 0.7\nStrong\n\n\n\n\n\n\nNow, based on Table 3, let change our Figure 6 to correlation heatmap\n\n\n\n\n\nFigure 7: correlation plot for big data\n\n\n\n\n\n\n\n\n\nFigure 8: correlation plot for Figure 5"
  },
  {
    "objectID": "content/data.html#time-series-data",
    "href": "content/data.html#time-series-data",
    "title": "About Data",
    "section": "Time Series Data",
    "text": "Time Series Data\n\nExample 1\nImagine you have time-series data as below.\n\n\n\n\n\n\n\nTable 4: Weather Dataset\n\n\n\nTemperature (C)\nApparent Temperature (C)\nHumidity\nWind Speed (km/h)\nWind Bearing (degrees)\nVisibility (km)\nLoud Cover\nPressure (millibars)\nDaily Summary\n\n\n\n\n0\n9.472222\n7.388889\n0.89\n14.1197\n251.0\n15.8263\n0.0\n1015.13\nPartly cloudy throughout the day.\n\n\n1\n9.355556\n7.227778\n0.86\n14.2646\n259.0\n15.8263\n0.0\n1015.63\nPartly cloudy throughout the day.\n\n\n2\n9.377778\n9.377778\n0.89\n3.9284\n204.0\n14.9569\n0.0\n1015.94\nPartly cloudy throughout the day.\n\n\n3\n8.288889\n5.944444\n0.83\n14.1036\n269.0\n15.8263\n0.0\n1016.41\nPartly cloudy throughout the day.\n\n\n4\n8.755556\n6.977778\n0.83\n11.0446\n259.0\n15.8263\n0.0\n1016.51\nPartly cloudy throughout the day.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: pair-plot for Table 4"
  },
  {
    "objectID": "content/intro-data.html",
    "href": "content/intro-data.html",
    "title": "Machine Learning Life-Cycle",
    "section": "",
    "text": "Machine Learning Life-Cycle\nSince Machine Learning (ML) is a collection of learning techniques used to extract value from data, we need to know what to say about data-at-hand, how machine learnt and how to evaluate learning model.\n\n\n\nFigure 1: ML Lifecycle1\n\n\n\n\nData source as vector\nIt is natural that we should seek to design and build machines that can recognize patterns. Consider an example in Figure 2. Suppose that we want to have a automate the process of detection whether the image is either “roti canai” or “chapati”. How do we do this?\n\n\n\nFigure 2: Typical information flow from data source to ML modelling\n\n\nFor an image, we need to get suitable “feature” from the source image and “teach” our system about the feature.\n\\[\n\\begin{equation*}\n\\textbf{x}=\n\\begin{bmatrix}\n1 \\\\\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix}\n\\end{equation*}\n\\] \\[\n\\begin{equation*}\n\\textbf{y}=\n\\begin{bmatrix}\n.3 \\\\\n-7\n\\end{bmatrix}\n\\end{equation*}\n\\] \\[\n\\begin{equation*}\n\\textbf{z}=\n\\begin{bmatrix}\n1 & 4 & 5 & 6\n\\end{bmatrix}\n\\end{equation*}\n\\]\n\\(\\textbf{x}\\) is a 4D column vector, \\(\\textbf{y}\\) is a 2D column vector, and \\(\\textbf{z}\\) is a 4D row vector.\nLinear algebra convention is to assume that vectors are in column orientation unless otherwise specified. Row vectors are written as \\(\\mathbf{w}^T\\). The \\(\\mathbf{}^T\\) indicates the transpose operation.\n\n\n\nFigure 3: 1 image(2D) to 1D\n\n\nLet’s consider a face like in Figure 3 be a two-dimensional \\(N\\) by \\(N\\) array of intensity values. This image may also be considered as a vector of dimension \\(N^{2}\\), so that a typical image of size 256 by 256 becomes a vector of dimension 65,536, or, equivalently, a point in 65,536-dimensional space.\n\n\n\nFigure 4: Time series to 1D\n\n\nFigure 4 on the hand show example of how time series data becomes a vector of 1D.\n\n\n\n\n\nReferences\n\n1. Watson, A. Synthetic data and the data-centric machine learning life cycle. (2022)."
  },
  {
    "objectID": "content/learning_1.html",
    "href": "content/learning_1.html",
    "title": "About Learning I",
    "section": "",
    "text": "To automatically discover regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.\n\n\n\nThe basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).\n\n\n\nProblem statement must be made in probability statement(s).\nGiven a data, can we test whether our statement is : TRUE or FALSE.\ne.g. Given an image cheque whose size is 1264x616 pixel, is image Standard Charted cheque or not?\ne.g. Given a 3 bedroom apartment in Kelana Jaya, will the resale price be between RM1,400/sqf and below RM1500/sqf?\ne.g. Given 4 to 5 parameter of atmospheric reading in Petaling Jaya, what is the chance of raining the next 3 hours?\nAll these statements are probability statements, which answers will be TRUE or FALSE; but the answer will always be in terms of probability.\ne.g. The probability of a image cheque whose size is 1264x616 pixel is image Standard Charted cheque is 43%. The probability for the resale price of the said apartment in the range will be 91.3%. The probability of raining in Petaling Jaya for the coming 3 hours is 63.5%.\n\n\n\nFrom a statistical learning point-of-view:\n\\[\nY = f(\\bf{X}) + \\epsilon\n\\] where \\({X}\\) is input variable:\n\\[\nX = (X_1,X_2,...,X_p)\n\\] and \\(\\epsilon\\) is noise.\nThe task is to get prediction/estimation of:\n\\[\n\\hat{Y} = \\hat{f}(\\bf{X}) + \\hat{\\epsilon}\n\\] The task is then relegated to error estimator, by defining Loss function:\n\\[\nError(x) = E[(Y−\\hat{f}(\\bf{X}))^2]\n\\]\n\n\n\nFigure 1: Visualize learning model\n\n\nA loss function, also known as a cost function, is a method used to estimate the discrepancies between the actual and predicted values in a machine learning model. It provides a measure of how well the model is performing.\nnoise in data refers to unwanted modifications introduced to a source signal during the capture, storage, transmission, or processing of its information.\n\n\n\n\n\n\n\n\nFigure 2: Example 1:Image data representation in vector\n\n\nBefore learning, data at hand must be representing in vector (e.g. Figure 2 & Figure 3)\n\n\n\nFigure 3: Example 2:Image data representation in vector\n\n\nBut wait, how about textual data ?\n\n\n\nFigure 4: Example 3:Text data representation in vector\n\n\n\n\n\nImagine you have a set of data. In order to have good estimated model, we have to split data at hand into:\n\nTraining set: This is the largest part in terms of the size of the dataset.\nValidation set: model training process is not a one-time process (highly iterative process). We have to train multiple models by trying different combinations of parameters (complexity). Then, we evaluate the performance of each model on the validation set.\nTest set: this is use after the training to evaluate performance of the model via un-seen data\n\n\n\n\nFigure 5: generic learning process\n\n\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\nFigure 6: training loss plot\n\n\n\n\n\nFigure 7: reference plot\n\n\n\n\n\nFigure 8: good plot\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization\n\n\n\n\n\nTo “learn” and achieve good generalization, how much data do we need?\n\n\n\nFigure 9: Effect of training data size on accuracy on medical images\n\n\nFigure 9 showed the effect of training data size on medical classification problem. From that report (see learning curve), learning classifier needs a training data set per class of 4092 to reach the desired accuracy, 99.5%.\n\n\n\nFigure 10: Effect of training data size on accuracy on cifar-10 images\n\n\nIn contrast, Figure 10 showed the effect of training data size on cifar-10 images problem. From that report (see learning curve), learning classifier needs a training data set per class of 6000 to reach the desired accuracy of above 90%.\n\n\n\nFigure 11: Effect of categories and instances size for YoloV5\n\n\nAnd for Figure 11 is the famous Yolo object detection classifier where number of images per category is recommended around ≥ 1500.\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n\n\n\n\n\nVijay Kotu and Bala Deshpande, Data Science Concepts and Practice, 2nd Ed., Elsevier Inc, 2019.\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nScott Fortmann-Roe, Understanding the Bias-Variance Tradeoff, 2012\nMachine Learning for Intelligent Systems: Bias-Variance Tradeoff\nML 101: Bias and Variance\nBias and Variance in Machine Learning\n\n\n\n\nWhat is learning? What does it mean for a computer to learn?\nThe big question of Learning Theory (and practice): how to get good generalization with a limited number of examples?"
  },
  {
    "objectID": "content/learning_1.html#big-picture",
    "href": "content/learning_1.html#big-picture",
    "title": "About Learning I",
    "section": "",
    "text": "To automatically discover regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories."
  },
  {
    "objectID": "content/learning_1.html#the-problem-setting-of-statistical-learning.",
    "href": "content/learning_1.html#the-problem-setting-of-statistical-learning.",
    "title": "About Learning I",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data)."
  },
  {
    "objectID": "content/learning_1.html#problem-statement",
    "href": "content/learning_1.html#problem-statement",
    "title": "About Learning I",
    "section": "",
    "text": "Problem statement must be made in probability statement(s).\nGiven a data, can we test whether our statement is : TRUE or FALSE.\ne.g. Given an image cheque whose size is 1264x616 pixel, is image Standard Charted cheque or not?\ne.g. Given a 3 bedroom apartment in Kelana Jaya, will the resale price be between RM1,400/sqf and below RM1500/sqf?\ne.g. Given 4 to 5 parameter of atmospheric reading in Petaling Jaya, what is the chance of raining the next 3 hours?\nAll these statements are probability statements, which answers will be TRUE or FALSE; but the answer will always be in terms of probability.\ne.g. The probability of a image cheque whose size is 1264x616 pixel is image Standard Charted cheque is 43%. The probability for the resale price of the said apartment in the range will be 91.3%. The probability of raining in Petaling Jaya for the coming 3 hours is 63.5%."
  },
  {
    "objectID": "content/learning_1.html#statistical-modelling",
    "href": "content/learning_1.html#statistical-modelling",
    "title": "About Learning I",
    "section": "",
    "text": "From a statistical learning point-of-view:\n\\[\nY = f(\\bf{X}) + \\epsilon\n\\] where \\({X}\\) is input variable:\n\\[\nX = (X_1,X_2,...,X_p)\n\\] and \\(\\epsilon\\) is noise.\nThe task is to get prediction/estimation of:\n\\[\n\\hat{Y} = \\hat{f}(\\bf{X}) + \\hat{\\epsilon}\n\\] The task is then relegated to error estimator, by defining Loss function:\n\\[\nError(x) = E[(Y−\\hat{f}(\\bf{X}))^2]\n\\]\n\n\n\nFigure 1: Visualize learning model\n\n\nA loss function, also known as a cost function, is a method used to estimate the discrepancies between the actual and predicted values in a machine learning model. It provides a measure of how well the model is performing.\nnoise in data refers to unwanted modifications introduced to a source signal during the capture, storage, transmission, or processing of its information."
  },
  {
    "objectID": "content/learning_1.html#how-to-learn",
    "href": "content/learning_1.html#how-to-learn",
    "title": "About Learning I",
    "section": "",
    "text": "Figure 2: Example 1:Image data representation in vector\n\n\nBefore learning, data at hand must be representing in vector (e.g. Figure 2 & Figure 3)\n\n\n\nFigure 3: Example 2:Image data representation in vector\n\n\nBut wait, how about textual data ?\n\n\n\nFigure 4: Example 3:Text data representation in vector\n\n\n\n\n\nImagine you have a set of data. In order to have good estimated model, we have to split data at hand into:\n\nTraining set: This is the largest part in terms of the size of the dataset.\nValidation set: model training process is not a one-time process (highly iterative process). We have to train multiple models by trying different combinations of parameters (complexity). Then, we evaluate the performance of each model on the validation set.\nTest set: this is use after the training to evaluate performance of the model via un-seen data\n\n\n\n\nFigure 5: generic learning process\n\n\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\nFigure 6: training loss plot\n\n\n\n\n\nFigure 7: reference plot\n\n\n\n\n\nFigure 8: good plot\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization"
  },
  {
    "objectID": "content/learning_1.html#how-much-data",
    "href": "content/learning_1.html#how-much-data",
    "title": "About Learning I",
    "section": "",
    "text": "To “learn” and achieve good generalization, how much data do we need?\n\n\n\nFigure 9: Effect of training data size on accuracy on medical images\n\n\nFigure 9 showed the effect of training data size on medical classification problem. From that report (see learning curve), learning classifier needs a training data set per class of 4092 to reach the desired accuracy, 99.5%.\n\n\n\nFigure 10: Effect of training data size on accuracy on cifar-10 images\n\n\nIn contrast, Figure 10 showed the effect of training data size on cifar-10 images problem. From that report (see learning curve), learning classifier needs a training data set per class of 6000 to reach the desired accuracy of above 90%.\n\n\n\nFigure 11: Effect of categories and instances size for YoloV5\n\n\nAnd for Figure 11 is the famous Yolo object detection classifier where number of images per category is recommended around ≥ 1500.\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants"
  },
  {
    "objectID": "content/learning_1.html#references",
    "href": "content/learning_1.html#references",
    "title": "About Learning I",
    "section": "",
    "text": "Vijay Kotu and Bala Deshpande, Data Science Concepts and Practice, 2nd Ed., Elsevier Inc, 2019.\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nScott Fortmann-Roe, Understanding the Bias-Variance Tradeoff, 2012\nMachine Learning for Intelligent Systems: Bias-Variance Tradeoff\nML 101: Bias and Variance\nBias and Variance in Machine Learning\n\n\n\n\nWhat is learning? What does it mean for a computer to learn?\nThe big question of Learning Theory (and practice): how to get good generalization with a limited number of examples?"
  },
  {
    "objectID": "content/learning_3.html",
    "href": "content/learning_3.html",
    "title": "About Learning III",
    "section": "",
    "text": "At the end of “training” (after model optimization), we need to ask the following:\n\nHow well is our model doing?\nIs our model good enough for us to use?\n\nTo answer this, we must do model evaluation. And to evaluate, certain measurement or metric is used to judge (evaluate) the performance of your model. It provides a more interpretable measure of your model’s performance.\nAlso recall that our “learning” output answer will always be in terms of probability.\n\n\nHere are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class.\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class\nFalse Positives (FP) is an outcome where the model incorrectly predicts the positive class.\nFalse Negatives (FN) is an outcome where the model incorrectly predicts the negative class.\n\n\n\n\nFigure 1: common general metric\n\n\nPrecision: This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall: metric that shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\n\n\n\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nRohit Kundu, Precision vs. Recall: Differences, Use Cases & Evaluation, 2022"
  },
  {
    "objectID": "content/learning_3.html#common-metrics",
    "href": "content/learning_3.html#common-metrics",
    "title": "About Learning III",
    "section": "",
    "text": "Here are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class.\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class\nFalse Positives (FP) is an outcome where the model incorrectly predicts the positive class.\nFalse Negatives (FN) is an outcome where the model incorrectly predicts the negative class.\n\n\n\n\nFigure 1: common general metric\n\n\nPrecision: This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall: metric that shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\n\n\n\nBishop, C. M., Pattern Recognition and Machine Learning, Springer, 2006\nWan Hasni, MD Labs Data Science Lecture Series, Techna-X, 2020\nRohit Kundu, Precision vs. Recall: Differences, Use Cases & Evaluation, 2022"
  }
]