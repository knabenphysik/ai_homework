{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "\n",
        "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform *`basic data exploration`* to study the basic characteristics of a dataset.\n",
        "\n",
        "By studying it basic properties, we may find useful patterns, connections, and relationships within data. This is usually called *`data exploration`* or *`exploratory data analysis (EDA)`*.\n",
        "\n",
        "The whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is ***good or not***.\n",
        "\n",
        "Once we have ***good data***, we can training our machine learning model and get ***accurate*** results.\n",
        "\n",
        "\n",
        "## Dataset Source Type\n",
        "\n",
        "Common type of dataset is:\n",
        "\n",
        "-   categorical data (image, voice, videos)\n",
        "-   tabular/numerical data\n",
        "-   time series\n",
        "-   text\n",
        "\n",
        "## Common Data pre-processing\n",
        "\n",
        "### Data transformation\n",
        "\n",
        "Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n",
        "\n",
        "### Data augmentation\n",
        "\n",
        "Data augmentation is used when we want to add “organized entropy” into an existing data set. This is only meaningful if those entropy helps to expand the data at hand, without altering the “meaningful aspects” of the data. That’s why it is useful in image processing or voice, which structures are not altered, but increase the space and dimensions.\n",
        "\n",
        "### Data de-noising\n",
        "\n",
        "Opposite to augmentation is “de-noising”, where we apply filters to take out the noises in the data. The argument here is reverse that is to reduce entropy in the data. Again, this is meaningful if there are no alterations to the basic structure of the data; and hence useful in image or voice processing. In NLP, removal of stop-words is a de-noising exercise.\n",
        "\n",
        "### Data pre-processing\n",
        "\n",
        "Data pre-processing may involve all of the above: transformation, augmentation and de-noising. In some cases all are required and helpful, in some cases a mixture of them will do. The basic process however is always data transformation.\n",
        "\n",
        "### Dimensionality  Reduction\n",
        "\n",
        "Dimensionality Reduction is a method of mapping a set of data onto a smaller space, represented by unique mapping between the raw data and a vector space, which serves as a “look-up table”. This reduction does not alter the structure of the data, instead it just compressed the data into a smaller space in terms of computer memories. Instead of working with raw data, we deal with its \"essence\" representations. An example of this is tokenization in NLP.\n",
        "\n",
        "### Missing-data imputation\n",
        "\n",
        "Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n",
        "\n",
        "### Data Duplicate\n",
        "\n",
        "To reduce model biased toward certain patterns in data, data duplicate should be remove.\n",
        "\n",
        "## Categorical Data\n",
        "### Example 1\n",
        "\n",
        "Imagine you have mnist image [data](https://www.kaggle.com/competitions/digit-recognizer/data) below.\n"
      ],
      "id": "a8130f3c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mnist\n",
        "#| fig-cap: sample images of mnist dataset\n",
        "#| echo: false\n",
        "\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "fig, ax = plt.subplots(8, 8, figsize=(6,6))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(digits.images[i], cmap='binary')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "plt.show()"
      ],
      "id": "fig-mnist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, since our data now is images and not tabular. We can use simple ***histogram plot*** like @fig-mnist-count to view each ***class*** of MNIST dataset.\n",
        "\n",
        "-   We can observe that data distribution `almost` the same (almost balanced) for each class\n"
      ],
      "id": "1f6bf5bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mnist-count\n",
        "#| fig-cap: images distribution of mnist dataset\n",
        "#| column: screen-inset-shaded\n",
        "#| echo: false\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "numberMNIST = fetch_openml('mnist_784',return_X_y=False)\n",
        "dataset = numberMNIST.data     \n",
        "labels = numberMNIST.target    \n",
        "\n",
        "X_train, X_test, Y_train, Y_test = dataset[:60000], dataset[60000:], labels[:60000], labels[60000:]\n",
        "\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(unique, counts);\n",
        "plt.xticks(unique);\n",
        "plt.xlabel(\"Label\");\n",
        "plt.ylabel(\"Quantity\");\n",
        "plt.title(\"Labels in MNIST 784 dataset\");"
      ],
      "id": "fig-mnist-count",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tabular Data\n",
        "\n",
        "-   scatter plot $\\Longrightarrow$ two variables are plotted along two axes.\n",
        "-   pairplot $\\Longrightarrow$ pairwise relationships between variables within a dataset\n",
        "\n",
        "The closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\n",
        "\n",
        "If a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: **positive** or **negative**, **strong** or **weak**.\n",
        "\n",
        "![correlation plot](image/corre.png){#fig-corre}\n",
        "\n",
        "So, in statistical terms we use correlation to denote association between two quantitative variables.\n",
        "\n",
        "### Example 1\n",
        "\n",
        "Imagine you have tabular [data](https://www.kaggle.com/competitions/titanic/data) as below.\n"
      ],
      "id": "02c9eac8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-dummy-data\n",
        "#| tbl-cap: Dummy Dataset\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data1 = pd.read_csv(\"data/dummy1.csv\")\n",
        "data1.head()"
      ],
      "id": "tbl-dummy-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset in @tbl-dummy-data have 7 columns $\\Longrightarrow$ 7 features\n",
        "\n",
        "Question : From @fig-corre, what is the best way to describe or visualize the data given to us? Answer: Let's do pair-plot (combination of scatter plot)\n"
      ],
      "id": "42829020"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 5,
        "fig-height": 4
      },
      "source": [
        "#| label: fig-pair-plot\n",
        "#| fig-cap: pair plot\n",
        "#| echo: false\n",
        "#| column: screen\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mplhep as hep\n",
        "import seaborn as sns\n",
        "hep.style.use(\"CMS\")\n",
        "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
        "plt.rcParams.update({'font.size': 7})\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "colors = iter(['xkcd:red purple', 'xkcd:pale teal', 'xkcd:warm purple',\n",
        "       'xkcd:light forest green', 'xkcd:blue with a hint of purple',\n",
        "       'xkcd:light peach', 'xkcd:dusky purple', 'xkcd:pale mauve',\n",
        "       'xkcd:bright sky blue', 'xkcd:baby poop green', 'xkcd:brownish',\n",
        "       'xkcd:moss green', 'xkcd:deep blue', 'xkcd:melon',\n",
        "       'xkcd:faded green', 'xkcd:cyan', 'xkcd:brown green',\n",
        "       'xkcd:purple blue', 'xkcd:baby shit green', 'xkcd:greyish blue'])\n",
        "\n",
        "def my_scatter(x,y, **kwargs):\n",
        "    kwargs['color'] = next(colors)\n",
        "    plt.scatter(x,y, **kwargs)\n",
        "\n",
        "def my_hist(x, **kwargs):\n",
        "    kwargs['color'] = next(colors)\n",
        "    plt.hist(x, **kwargs)\n",
        "\n",
        "g = sns.PairGrid(data1, corner=True)\n",
        "g.map_diag(my_hist)\n",
        "g.map_offdiag(my_scatter)"
      ],
      "id": "fig-pair-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What can we say about @fig-pair-plot ?\n"
      ],
      "id": "d678a8f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 5,
        "fig-height": 4
      },
      "source": [
        "#| label: fig-big-pair-plot\n",
        "#| fig-cap: big pair plot\n",
        "#| echo: false\n",
        "#| column: screen-inset-shaded\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mplhep as hep\n",
        "import seaborn as sns\n",
        "hep.style.use(\"CMS\")\n",
        "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
        "plt.rcParams.update({'font.size': 7})\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data2 = pd.read_csv(\"data/dummy2.csv\", usecols=lambda x: x != 'feature_A')\n",
        "sns.pairplot(data2, corner=True);"
      ],
      "id": "fig-big-pair-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2\n"
      ],
      "id": "5bed6e6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-big-dummy-data\n",
        "#| tbl-cap: Dummy Big Dataset\n",
        "#| echo: false\n",
        "\n",
        "data2.head()"
      ],
      "id": "tbl-big-dummy-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if we have many features like @tbl-big-dummy-data and want to plot pair-plot like @fig-big-pair-plot, seem to overwhelming and confuse isn't?\n",
        "\n",
        "Solution? Use correlation heatmap $\\Longrightarrow$ easier to see based on correlation value/coefficient, r (recall our @fig-corre).r value is the degree of association.\n"
      ],
      "id": "8aeedf31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-correlation\n",
        "#| tbl-cap: correlation guide\n",
        "#| echo: false\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from tabulate import tabulate\n",
        "table = [[\"r < 0.3\", \"None or very weak\"],\n",
        "         [\"0.3 < r <0.5\", \"Weak\"],\n",
        "         [\"0.5 < r < 0.7\", \"Moderate\"],\n",
        "         [\"r > 0.7\", \"Strong\"]]\n",
        "Markdown(tabulate(\n",
        "  table, \n",
        "  headers=[\"Correlation Value (r)\", \"Strength of Relationship\"]\n",
        "))"
      ],
      "id": "tbl-correlation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, based on @tbl-correlation, let change our @fig-big-pair-plot to correlation heatmap\n"
      ],
      "id": "e7c38be9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-big-correlation-plot\n",
        "#| fig-cap: correlation plot for big data\n",
        "#| echo: false\n",
        "\n",
        "data2 = data2.replace(['rain'],['0'])\n",
        "data2 = data2.replace(['Rain, Partially cloudy'],['0'])\n",
        "data2 = data2.replace(['Partially cloudy'],['1'])\n",
        "data2 = data2.replace(['Rain, Overcast'],['2'])\n",
        "data2 = data2.replace(['Overcast'],['3'])\n",
        "data2 = data2.replace(['partly-cloudy-night'],['4'])\n",
        "data2 = data2.replace(['cloudy'],['5'])\n",
        "data2 = data2.replace(['Rain'],['6'])\n",
        "data2 = data2.replace(['Clear'],['7'])\n",
        "data2 = data2.replace(['partly-cloudy-day'],['8'])\n",
        "data2 = data2.replace(['clear-night'],['9'])\n",
        "data2 = data2.replace(['clear-day'],['10'])\n",
        "\n",
        "corr = data2.corr(method='spearman')\n",
        "\n",
        "f,ax = plt.subplots(figsize=(9,6))\n",
        "sns.heatmap(corr, annot = True, fmt='.2g',cmap= 'coolwarm', ax=ax)\n",
        "# plt.subplot(1, 2, 1)\n",
        "ax.set_title ('Correlation Matrix for big data', fontdict = {'fontsize':10}, pad = 10);\n",
        "plt.show()"
      ],
      "id": "fig-big-correlation-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-small-correlation-plot\n",
        "#| fig-cap: correlation plot for @fig-pair-plot\n",
        "#| echo: false\n",
        "\n",
        "corr_ = data1.corr(method='spearman')\n",
        "\n",
        "f,ax = plt.subplots(figsize=(9,6))\n",
        "sns.heatmap(corr_, annot = True, fmt='.2g',cmap= 'coolwarm', ax=ax)\n",
        "# plt.subplot(1, 2, 1)\n",
        "ax.set_title ('Correlation Matrix for small data', fontdict = {'fontsize':10}, pad = 10);\n",
        "plt.show()"
      ],
      "id": "fig-small-correlation-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Data\n",
        "### Example 1\n",
        "\n",
        "Imagine you have time-series [data](https://www.kaggle.com/datasets/muthuj7/weather-dataset/data) as below.\n"
      ],
      "id": "8e159168"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-weather-data\n",
        "#| tbl-cap: Weather Dataset\n",
        "#| echo: false\n",
        "\n",
        "cuaca = pd.read_csv(\"data/weatherHistory.csv\")\n",
        "df_new_num = cuaca.drop(['Formatted Date','Summary','Precip Type' ], axis=1)\n",
        "df_new_num.head()"
      ],
      "id": "tbl-weather-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-weather-correlation-plot\n",
        "#| fig-cap: pair-plot for @tbl-weather-data\n",
        "#| echo: false\n",
        "\n",
        "sns.pairplot(cuaca, hue='Precip Type',palette='GnBu')"
      ],
      "id": "fig-weather-correlation-plot",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}