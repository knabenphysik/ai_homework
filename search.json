[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "These lecture notes aim to gives a short introduction into the basic ideas and concepts of artificial intelligence (AI). The approach and selection of topics reflect my experience with AI for the past 15 years. Most of my knowledge about the subject was self-taught through online courses, side projects and professional activities. Since my background was in applied physics,basis for getting into machine learning for natural and easy.\nI hope that this notes will be useful for self-study and as a companion for more formal AI course offered elsewhere.\nI have tried throughout to minimize rigorous mathematics thus making the note as accessible and self-contained as possible. Some relevant background material is provided through appendices or ‘narrative summaries’ within the main text, together with pointers to the literature.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "href": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "title": "Preface",
    "section": "Artificial intelligence, Machine learning and Deep learning",
    "text": "Artificial intelligence, Machine learning and Deep learning\nData science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\n\nFigure 16: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 17: Artificial intelligence, machine learning, and data science.4\n\n\n\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\n\n\n\n\n\n\nHow do we relate it all?\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\n\n\nMachine learning tends to be specifically interested in pattern recognition and learning from data. Artificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-scientist-vs-machine-learning-engineer",
    "href": "index.html#data-scientist-vs-machine-learning-engineer",
    "title": "Preface",
    "section": "Data Scientist vs Machine Learning Engineer",
    "text": "Data Scientist vs Machine Learning Engineer\n\n\n\n\n\n\nFigure 18: Domain area of deep learning5\n\n\n\nMLOps is the process of automating and productize machine learning applications and workflows.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reality-ml-in-production",
    "href": "index.html#reality-ml-in-production",
    "title": "Preface",
    "section": "Reality: ML in production",
    "text": "Reality: ML in production\nMachine learning in production is very complicated!\n\n\n\n\n\n\nFigure 19: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\nIn a perfect world, data scientist will do ML modelling while ML Engineer will productize ML model from Data Scientist. In reality (especially in Small & Medium Enterprise), Data Scientist & ML Engineer job scope is intertwine (or even the same person!)\n\n\n\n\n\n\nFigure 20: Categories of job titles related to AI\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\n\n\nFormulate a definition of intelligence. How do we test this intelligence empirically ( testable and verifiable by observation)?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/license.html",
    "href": "content/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "Text and figures are licensed under Creative Commons Attribution CC BY-SA 4.0. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”."
  },
  {
    "objectID": "content/08_exciting_ai.html",
    "href": "content/08_exciting_ai.html",
    "title": "Exciting AI",
    "section": "",
    "text": "Exciting AI\nWhat is exciting AI today?\n\ntransformer\nLLM\nimage generation",
    "crumbs": [
      "08 - Exciting AI Today"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html",
    "href": "content/06_machine_learning.html",
    "title": "What is Machine Learning",
    "section": "",
    "text": "What is algorithm?\n\n\n\nInformally, an algorithm is any well-defined computational procedure that takes some value, or set of values, as input and produces some values, or set of values, as output. An algorithm is thus a sequence of computational steps that transform the input into the output.\n\n\n\n\nIn the context of prediction, nature can be seen as a mechanism that takes features \\(X\\) and produces output \\(Y\\) . This mechanism is unknown, and modelers “fill” the gap with models.\n\n\n\n\n\n\nFigure 1: Modelling nature\n\n\n\nStatistical modelers fill this box with a statistical model. If the modeler is convinced that the model represents the data-generating process well, they can interpret the model and parameters as parameters of nature. Since nature’s true mechanism is unknown and not fully specified by the data, modelers have to make some assumptions.\n\n\n\n\n\n\nFigure 2: Statistical Model\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Type of ML\n\n\n\n\n\n\nIn supervised learning, nature is seen as unknowable. Instead of the intrinsic approach, supervised learning takes an extrinsic approach. The supervised model is supposed to mimic the outputs of nature, but it doesn’t matter whether the model’s inner mechanism is a reasonable substitute for nature’s mechanism.\n\n\n\n\n\n\nFigure 4: Supervised Learning Model\n\n\n\nA statistical modeler would try to find a plausible recipe, even if the end result is not perfect. The supervised learner would only be interested in the end result; it doesn’t matter whether it was exactly the same recipe.\nIs to map input onto sets of output, which had been pre-determined by the “supervisor” or researcher/analyzer/modeler. Hence called “supervised”. The problem is \\(X \\mapsto Y\\). In another word, we have \\(X\\) as input space, and \\(Y\\) output space, and the learning is a process of mapping from \\(X\\) to \\(Y\\). The learner receives a set of labeled examples as training data and makes predictions for all unseen points.\n\n\n\nThe goal of regression is to predict the value of one or more continuous target variables \\(t\\) given the value of a D-dimensional vector \\(x\\) of input variables. Usually, predict a numerical value.\nGiven a training data set comprising \\(N\\) observations \\(\\{x_n \\}\\), where \\(n = 1, . . . , N ,\\) together with corresponding target values \\(\\{t_n\\}\\), the goal is to predict the value of \\(t\\) for a new value of \\(x\\).\n\n\n\n\n\n\nFigure 5: Regression Learning\n\n\n\nExample:\n\nWhat will the temperature for tomorrow morning?\nWhat is Kelana Park View apartment current selling prices?\npredict water temperature based on salinity\npredict the apparent temperature given the humidity\nstock price predict of Tesla Inc\n\nTypes of Regression Algorithm:\n\nSimple Linear Regression\n\n\n\n\nThe goal in classification is to take an input vector \\(x\\) and to assign it to one of \\(K\\) discrete classes \\(\\{C_k \\}\\) where \\(k = 1, . . . , K\\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries. Classify into two or multiple classes.\n\n\n\n\n\n\nFigure 6: Classification Learning\n\n\n\nExamples:\n\nTomorrow morning raining or sunny?\nImage classification\nDocument classification\nFacial/Voice recognition\nText classification\n\nTypes of Classification Algorithm:\n\nNeural network\nsupport vector machine\nNaive Bayes\nK-Nearest Neighbors\nDecision Tree Classifier\nRandom Forest Classifier\n\n\n\n\n\n\n\nCurse of dimensionality\n\n\n\nWith the rapid development of data collection and storage technologies, we have entered the era of big data, where the data holds a trend of rapid growth in both sample size and feature dimensionality.\nAs we add more dimensions(features) to our dataset, the volume of the space increases exponentially. This means that the data becomes sparse.\nThis sparsity making it hard to find patterns or relationships (lack of representativeness). It also will increase computation cost.\n\n\n\n\n\nUnsupervised learning means discovering hidden patterns in data. Unsupervised learning is more like, “Here are some data points. Please find something interesting.” The learner exclusively receives unlabeled training data, and makes predictions for all unseen points.\nWe have only \\(Xs\\), input data, and no output to map to. \\(Y\\) may exist, but it is “latent” (i.e. not directly observable)\n\n\n\n\n\n\nFigure 7: Cluster Learning\n\n\n\nExamples:\n\nanomaly detection\ncustomer segmentation\n\n\n\n\nReinforcement learning is dynamic learning.\nThe output \\(Y\\) is a set of actions, which is ordered in a sequence \\((A_i,A_j,A_k,...,A_n)\\). The objective is to “learn” which set of actions sequence which maximizes the rewards, \\(R\\). \\(S\\) now is called the possible states or set of states. \\(Y\\) is the game theoretic action space choosing the states, and \\(Z\\) is the reward space, given various state space sequences or paths.\n\n\n\n\n\n\nFigure 8: Reinforcement Learning\n\n\n\nExamples:\n\nself-driving cars\nindustrial robotics\nchatGPT\n\n\n\n\n\nSession with python and relevant libraries.\n\n\n\nA popular definition of interpretability frequently used by researchers is “interpretability in machine learning is a degree to which a human can understand the cause of a decision from an ML model”. It can also be defined as “the ability to explain the model outcome in understandable ways to a human. So far, no mathematical definition. The primary goal of interpretability is to explain the model outcomes in a way that is easy for users to understand.\n\n\n\n\n\n\nFigure 9: Sampling from big data\n\n\n\nInterpretability — If a business wants high model transparency and wants to understand exactly why and how the model is generating predictions, they need to observe the inner mechanics of the AI/ML method. This leads to interpreting the model’s weights and features to determine the given output. This is interpretability.\nHigh interpretability typically comes at the cost of performance, as seen in the following figure. If we wants to achieve high performance but still wants to have a general understanding of the model behavior, model explainability starts to play a larger role.\nExplainability — Explainability is how to take an ML model and explain the behavior in human terms.\nWhen datasets are large and the data is related to images or text, neural networks can meet the customer’s AI/ML objective with high performance. In such cases, where complex methods are required to maximize performance, data scientists may focus on model explainability instead of interpretability.\nFlexibility — There’s no rigor definition of method’s flexibility. Flexibility describes the ability to increase the degrees of freedom available to the model to “fit” to the training data.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#mimic-outputs-not-the-process",
    "href": "content/06_machine_learning.html#mimic-outputs-not-the-process",
    "title": "What is Machine Learning",
    "section": "",
    "text": "In the context of prediction, nature can be seen as a mechanism that takes features \\(X\\) and produces output \\(Y\\) . This mechanism is unknown, and modelers “fill” the gap with models.\n\n\n\n\n\n\nFigure 1: Modelling nature\n\n\n\nStatistical modelers fill this box with a statistical model. If the modeler is convinced that the model represents the data-generating process well, they can interpret the model and parameters as parameters of nature. Since nature’s true mechanism is unknown and not fully specified by the data, modelers have to make some assumptions.\n\n\n\n\n\n\nFigure 2: Statistical Model\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Type of ML\n\n\n\n\n\n\nIn supervised learning, nature is seen as unknowable. Instead of the intrinsic approach, supervised learning takes an extrinsic approach. The supervised model is supposed to mimic the outputs of nature, but it doesn’t matter whether the model’s inner mechanism is a reasonable substitute for nature’s mechanism.\n\n\n\n\n\n\nFigure 4: Supervised Learning Model\n\n\n\nA statistical modeler would try to find a plausible recipe, even if the end result is not perfect. The supervised learner would only be interested in the end result; it doesn’t matter whether it was exactly the same recipe.\nIs to map input onto sets of output, which had been pre-determined by the “supervisor” or researcher/analyzer/modeler. Hence called “supervised”. The problem is \\(X \\mapsto Y\\). In another word, we have \\(X\\) as input space, and \\(Y\\) output space, and the learning is a process of mapping from \\(X\\) to \\(Y\\). The learner receives a set of labeled examples as training data and makes predictions for all unseen points.\n\n\n\nThe goal of regression is to predict the value of one or more continuous target variables \\(t\\) given the value of a D-dimensional vector \\(x\\) of input variables. Usually, predict a numerical value.\nGiven a training data set comprising \\(N\\) observations \\(\\{x_n \\}\\), where \\(n = 1, . . . , N ,\\) together with corresponding target values \\(\\{t_n\\}\\), the goal is to predict the value of \\(t\\) for a new value of \\(x\\).\n\n\n\n\n\n\nFigure 5: Regression Learning\n\n\n\nExample:\n\nWhat will the temperature for tomorrow morning?\nWhat is Kelana Park View apartment current selling prices?\npredict water temperature based on salinity\npredict the apparent temperature given the humidity\nstock price predict of Tesla Inc\n\nTypes of Regression Algorithm:\n\nSimple Linear Regression\n\n\n\n\nThe goal in classification is to take an input vector \\(x\\) and to assign it to one of \\(K\\) discrete classes \\(\\{C_k \\}\\) where \\(k = 1, . . . , K\\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries. Classify into two or multiple classes.\n\n\n\n\n\n\nFigure 6: Classification Learning\n\n\n\nExamples:\n\nTomorrow morning raining or sunny?\nImage classification\nDocument classification\nFacial/Voice recognition\nText classification\n\nTypes of Classification Algorithm:\n\nNeural network\nsupport vector machine\nNaive Bayes\nK-Nearest Neighbors\nDecision Tree Classifier\nRandom Forest Classifier\n\n\n\n\n\n\n\nCurse of dimensionality\n\n\n\nWith the rapid development of data collection and storage technologies, we have entered the era of big data, where the data holds a trend of rapid growth in both sample size and feature dimensionality.\nAs we add more dimensions(features) to our dataset, the volume of the space increases exponentially. This means that the data becomes sparse.\nThis sparsity making it hard to find patterns or relationships (lack of representativeness). It also will increase computation cost.\n\n\n\n\n\nUnsupervised learning means discovering hidden patterns in data. Unsupervised learning is more like, “Here are some data points. Please find something interesting.” The learner exclusively receives unlabeled training data, and makes predictions for all unseen points.\nWe have only \\(Xs\\), input data, and no output to map to. \\(Y\\) may exist, but it is “latent” (i.e. not directly observable)\n\n\n\n\n\n\nFigure 7: Cluster Learning\n\n\n\nExamples:\n\nanomaly detection\ncustomer segmentation\n\n\n\n\nReinforcement learning is dynamic learning.\nThe output \\(Y\\) is a set of actions, which is ordered in a sequence \\((A_i,A_j,A_k,...,A_n)\\). The objective is to “learn” which set of actions sequence which maximizes the rewards, \\(R\\). \\(S\\) now is called the possible states or set of states. \\(Y\\) is the game theoretic action space choosing the states, and \\(Z\\) is the reward space, given various state space sequences or paths.\n\n\n\n\n\n\nFigure 8: Reinforcement Learning\n\n\n\nExamples:\n\nself-driving cars\nindustrial robotics\nchatGPT",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#example-session",
    "href": "content/06_machine_learning.html#example-session",
    "title": "What is Machine Learning",
    "section": "",
    "text": "Session with python and relevant libraries.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#interpretability-vs-flexibility",
    "href": "content/06_machine_learning.html#interpretability-vs-flexibility",
    "title": "What is Machine Learning",
    "section": "",
    "text": "A popular definition of interpretability frequently used by researchers is “interpretability in machine learning is a degree to which a human can understand the cause of a decision from an ML model”. It can also be defined as “the ability to explain the model outcome in understandable ways to a human. So far, no mathematical definition. The primary goal of interpretability is to explain the model outcomes in a way that is easy for users to understand.\n\n\n\n\n\n\nFigure 9: Sampling from big data\n\n\n\nInterpretability — If a business wants high model transparency and wants to understand exactly why and how the model is generating predictions, they need to observe the inner mechanics of the AI/ML method. This leads to interpreting the model’s weights and features to determine the given output. This is interpretability.\nHigh interpretability typically comes at the cost of performance, as seen in the following figure. If we wants to achieve high performance but still wants to have a general understanding of the model behavior, model explainability starts to play a larger role.\nExplainability — Explainability is how to take an ML model and explain the behavior in human terms.\nWhen datasets are large and the data is related to images or text, neural networks can meet the customer’s AI/ML objective with high performance. In such cases, where complex methods are required to maximize performance, data scientists may focus on model explainability instead of interpretability.\nFlexibility — There’s no rigor definition of method’s flexibility. Flexibility describes the ability to increase the degrees of freedom available to the model to “fit” to the training data.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html",
    "href": "content/04_learning.html",
    "title": "About Learning I",
    "section": "",
    "text": "“The answers you get depend upon the questions you ask.” - Thomas Kuhn.\n\n\n\nLearning in Machine Learning is learning via computational methods using experience (data) to improve performance or to make accurate predictions.1\nThis learning is to automatically (minimal human intervention) discover regularities/patterns in data (through the use of computer algorithms) and with the use of these regularities to take actions (e.g. classifying the data into different categories)\n\n\n\nStatistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.2\nAnother way of understanding statistical learning is, it’s about studying the concept of inference (process of drawing conclusions from data) in both supervised and unsupervised machine learning. Inference covers the entire spectrum of machine learning, from gaining knowledge, making predictions or decisions and constructing models from a set of labeled or unlabeled data.\n\n\n\n\n\n\nFigure 1: Different learning strategy\n\n\n\n\nSupervised Learning: learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into certain task (e.g. classifying data or predicting outcomes accurately)\nUnsupervised Learning: learning strategy that’s looking for hidden or underlying patterns in datasets.\nReinforcement Learning: learning strategy which does not require labeled data or a training set. It relies on the ability to monitor the response to the actions of the learning agent. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n\nAt the end we will measure success of this learning by some statistical model performance. This will tell whether our “trained” model is generalize(model’s ability to adapt properly to new, previously unseen data) well or not,\n\n\n\n\n\n\nFigure 2: Current AI Landscape\n\n\n\nFigure 2 shows current AI opportunities. Andrew Ng highlight the opportunities in:\n\nAI for productivity : usage of Large Language Model (LLM) for office work\nAI for new products and services : to utilised AI to build previously unimaginable services and products.\n\n\nWe shall focus on Supervised Learning\n\n\n\n\nThe basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).\n\n\n\nVectors and matrices are fundamental mathematical structures that serve as the building blocks for representing and manipulating data in machine learning. They provide a concise and efficient way to organize, transform, and analyze complex datasets, enabling machine learning algorithms to make sense of raw information and extract meaningful patterns.\n\nscalar: just a number, like 7, 42, \\(\\pi\\). To a computer, a scalar is a simple numeric variable\nvector: 1D array of numbers. Mathematically, a vector has an orientation, either horizontal or vertical. If horizontal, it’s a row vector. For example:\n\n\\[\\textbf x = \\begin{bmatrix} x_0 & x_1 & x_3 \\end{bmatrix}\\] Mathematically, vectors are usually assumed to be column vectors:\n\\[\\textbf y = \\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\\] Either notation is acceptable. As we recall from SPM, vector usually have component (x,y and z) to represent a point in 3D space. However, in deep learning, and machine learning in general, vector are used to represent features, qualities of some sample that the model will use to attempt to arrive at a useful output, like a class label, or a regression value.\n\nmatrices: 2D array of numbers:\n\n\\[\\textbf A = \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12}  \\end{bmatrix}\\]\n\n\n\nWe will use the problem of car image classification as a running example to illustrate some basic definitions and to describe the use and evaluation of machine learning algorithms in practice. car image classification is the problem of learning to automatically classify 2 type of car image messages as either Perodua MyVi or Proton Iriz.\n\n\n\nLabel\nDescription\n\n\n\n\n0\nMyVi\n\n\n1\nIriz\n\n\n\n\nExamples: Items or instances of data used for learning or evaluation. In our problem, these examples correspond to the collection of car images we will use for learning and testing.\nFeatures: The set of attributes, often represented as a vector, associated to an example. In the case of image classification, pixels are the raw building blocks of an image. Every image consists of a set of pixels. There is no finer granularity than the pixel. A pixel is considered the “color” or the “intensity” of light that appears in a given place in our image. Various location of this color will give texture, and shape.\n\n\n\n\n\n\n\nFigure 3: Image data representation in vector\n\n\n\n\nLabels: Values or categories assigned to examples. In classification problems, examples are assigned specific categories, for instance, the Perodua MyVi and Proton Iriz categories in our binary classification problem.\nTraining sample: Examples used to train a learning algorithm. In our problem, the training sample consists of a set of car image examples along with their associated labels.\nValidation sample: Examples used to tune the parameters of a learning algorithm when working with labeled data. Learning algorithms typically have one or more free parameters, and the validation sample is used to select appropriate values for these model parameters.\nTest sample: Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available (unseen data) in the learning stage.\nLoss function: A function that measures the difference, or loss, between a predicted label and a true label. Also termed as objective function.\n\\(h\\), (hypothesis): an instance or specific candidate model that maps inputs to outputs and can be evaluated and used to make predictions. A hypothesis is like a smart(well-informed) guess.\n\\(H\\), Hypothesis set: set of conditions of specific candidate model that determines the classification into a group (a function approximation for the target function). It is a statement about the input data and its relation to the class. These “function” is used to associate/estimate or predict the target value, based on the input dataset, algorithm parameters (and hyper-parameters). Mathematically, it is a set of functions mapping features (feature vectors) to the set of labels Y. In our example, these may be a set of functions mapping car image features to Y = {MyVi, Iriz}.\nmodel: A model can be viewed as a probabilistic model or as a non-probabilistic model(function). Every model has its own set of parameters(the weights and bias).\n\n\nLearning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set.3\n\n\n\n\n\n\n\nFigure 4: generic learning process\n\n\n\nIn Figure 4, we show generic statistical learning process.To ensure that our statistical model is going to perform well on new data, we need to consider the following:\n\nUsing “user” prior knowledge, map relevant features of the collection of image into label/category\nPartition the data into a training sample, a validation sample, and a test sample.\nDuring training, example are drawn independently at random\nOur Machine learning algorithm now analyze the examples and produce a classifier,\\(f\\)\ngiven new example from validation, classifier,\\(f\\) predict \\(\\hat{y}=f(x)\\)\nMeasure loss between real truth, \\(y\\) and predicted,\\(\\hat{y}\\)\nIf loss high, repeat (3)-(6) until loss is minimize.\n\n\nWhat happen at step (4) - step (6)\n\nTo have rough idea what’s going on at step (4), consider Figure 5 where our data is label as vector(step (1)).\n\n\n\n\n\n\nFigure 5: data label\n\n\n\n\n\n\n\n\n\nFigure 6: What happened during training\n\n\n\nThus in Figure 6, during 1 individual training loop. Prediction vector is later will be compare with data label using loss function and error is compute.\n\n\n\n\nBias : amount that a model’s prediction differs from the target value, compared to the training data.\nLow Bias: fewer strong assumptions or constraints placed during model training. In this case, the model will closely match the training dataset. Model has ability to capture the true underlying patterns or relationships present in the data\nHigh Bias : model simplifies the underlying patterns too much (strong assumptions or constraints placed during model training). In this case, the model will not match the training dataset closely. Usually called under-fitting\nVariance: the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\nLow Variance : not overly influenced by the noise or fluctuations in the training data. It tends to be more stable, providing consistent predictions even when trained on different subsets of the data.\nHigh Variance : overly sensitive to the noise or fluctuations in the training data. It tries to fit the training data so closely that it captures not only the true underlying patterns but also the random noise in the data. Usually called over-fitting\nHigh Bias and Low Variance: Consistent but inaccurate due to oversimplification. \\(\\Longrightarrow\\) poor performance\nHigh Variance and Low Bias: Accurate but inconsistent, capturing noise and over-fitting to the training data. \\(\\Longrightarrow\\) poor performance\n\n\n\n\n\n\n\nFigure 7: What happened during training\n\n\n\nOverfitting is when your model is basically memorising the data without really understanding/interpolating a general function that would apply to any external data. In other words it creates a function that is tailored specifically to the training dataset thus it would perform near perfectly with the training data, but it would perform quite badly with any data that it hasn’t seen before. (models the training data too well but fails to generalize to new data)\nUnderfitting is simply when you do not have enough data for the model to create/interpolate a general function that describes the process. This leads to the model performing badly for most input data. (model that fails to capture the underlying pattern in the training data)\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 8: Training trade-off4\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#big-picture",
    "href": "content/04_learning.html#big-picture",
    "title": "About Learning I",
    "section": "",
    "text": "Learning in Machine Learning is learning via computational methods using experience (data) to improve performance or to make accurate predictions.1\nThis learning is to automatically (minimal human intervention) discover regularities/patterns in data (through the use of computer algorithms) and with the use of these regularities to take actions (e.g. classifying the data into different categories)",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#statistical-learning",
    "href": "content/04_learning.html#statistical-learning",
    "title": "About Learning I",
    "section": "",
    "text": "Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.2\nAnother way of understanding statistical learning is, it’s about studying the concept of inference (process of drawing conclusions from data) in both supervised and unsupervised machine learning. Inference covers the entire spectrum of machine learning, from gaining knowledge, making predictions or decisions and constructing models from a set of labeled or unlabeled data.\n\n\n\n\n\n\nFigure 1: Different learning strategy\n\n\n\n\nSupervised Learning: learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into certain task (e.g. classifying data or predicting outcomes accurately)\nUnsupervised Learning: learning strategy that’s looking for hidden or underlying patterns in datasets.\nReinforcement Learning: learning strategy which does not require labeled data or a training set. It relies on the ability to monitor the response to the actions of the learning agent. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n\nAt the end we will measure success of this learning by some statistical model performance. This will tell whether our “trained” model is generalize(model’s ability to adapt properly to new, previously unseen data) well or not,\n\n\n\n\n\n\nFigure 2: Current AI Landscape\n\n\n\nFigure 2 shows current AI opportunities. Andrew Ng highlight the opportunities in:\n\nAI for productivity : usage of Large Language Model (LLM) for office work\nAI for new products and services : to utilised AI to build previously unimaginable services and products.\n\n\nWe shall focus on Supervised Learning",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "href": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "title": "About Learning I",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#vectors-and-matrices",
    "href": "content/04_learning.html#vectors-and-matrices",
    "title": "About Learning I",
    "section": "",
    "text": "Vectors and matrices are fundamental mathematical structures that serve as the building blocks for representing and manipulating data in machine learning. They provide a concise and efficient way to organize, transform, and analyze complex datasets, enabling machine learning algorithms to make sense of raw information and extract meaningful patterns.\n\nscalar: just a number, like 7, 42, \\(\\pi\\). To a computer, a scalar is a simple numeric variable\nvector: 1D array of numbers. Mathematically, a vector has an orientation, either horizontal or vertical. If horizontal, it’s a row vector. For example:\n\n\\[\\textbf x = \\begin{bmatrix} x_0 & x_1 & x_3 \\end{bmatrix}\\] Mathematically, vectors are usually assumed to be column vectors:\n\\[\\textbf y = \\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\\] Either notation is acceptable. As we recall from SPM, vector usually have component (x,y and z) to represent a point in 3D space. However, in deep learning, and machine learning in general, vector are used to represent features, qualities of some sample that the model will use to attempt to arrive at a useful output, like a class label, or a regression value.\n\nmatrices: 2D array of numbers:\n\n\\[\\textbf A = \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12}  \\end{bmatrix}\\]",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#example",
    "href": "content/04_learning.html#example",
    "title": "About Learning I",
    "section": "",
    "text": "We will use the problem of car image classification as a running example to illustrate some basic definitions and to describe the use and evaluation of machine learning algorithms in practice. car image classification is the problem of learning to automatically classify 2 type of car image messages as either Perodua MyVi or Proton Iriz.\n\n\n\nLabel\nDescription\n\n\n\n\n0\nMyVi\n\n\n1\nIriz\n\n\n\n\nExamples: Items or instances of data used for learning or evaluation. In our problem, these examples correspond to the collection of car images we will use for learning and testing.\nFeatures: The set of attributes, often represented as a vector, associated to an example. In the case of image classification, pixels are the raw building blocks of an image. Every image consists of a set of pixels. There is no finer granularity than the pixel. A pixel is considered the “color” or the “intensity” of light that appears in a given place in our image. Various location of this color will give texture, and shape.\n\n\n\n\n\n\n\nFigure 3: Image data representation in vector\n\n\n\n\nLabels: Values or categories assigned to examples. In classification problems, examples are assigned specific categories, for instance, the Perodua MyVi and Proton Iriz categories in our binary classification problem.\nTraining sample: Examples used to train a learning algorithm. In our problem, the training sample consists of a set of car image examples along with their associated labels.\nValidation sample: Examples used to tune the parameters of a learning algorithm when working with labeled data. Learning algorithms typically have one or more free parameters, and the validation sample is used to select appropriate values for these model parameters.\nTest sample: Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available (unseen data) in the learning stage.\nLoss function: A function that measures the difference, or loss, between a predicted label and a true label. Also termed as objective function.\n\\(h\\), (hypothesis): an instance or specific candidate model that maps inputs to outputs and can be evaluated and used to make predictions. A hypothesis is like a smart(well-informed) guess.\n\\(H\\), Hypothesis set: set of conditions of specific candidate model that determines the classification into a group (a function approximation for the target function). It is a statement about the input data and its relation to the class. These “function” is used to associate/estimate or predict the target value, based on the input dataset, algorithm parameters (and hyper-parameters). Mathematically, it is a set of functions mapping features (feature vectors) to the set of labels Y. In our example, these may be a set of functions mapping car image features to Y = {MyVi, Iriz}.\nmodel: A model can be viewed as a probabilistic model or as a non-probabilistic model(function). Every model has its own set of parameters(the weights and bias).\n\n\nLearning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set.3\n\n\n\n\n\n\n\nFigure 4: generic learning process\n\n\n\nIn Figure 4, we show generic statistical learning process.To ensure that our statistical model is going to perform well on new data, we need to consider the following:\n\nUsing “user” prior knowledge, map relevant features of the collection of image into label/category\nPartition the data into a training sample, a validation sample, and a test sample.\nDuring training, example are drawn independently at random\nOur Machine learning algorithm now analyze the examples and produce a classifier,\\(f\\)\ngiven new example from validation, classifier,\\(f\\) predict \\(\\hat{y}=f(x)\\)\nMeasure loss between real truth, \\(y\\) and predicted,\\(\\hat{y}\\)\nIf loss high, repeat (3)-(6) until loss is minimize.\n\n\nWhat happen at step (4) - step (6)\n\nTo have rough idea what’s going on at step (4), consider Figure 5 where our data is label as vector(step (1)).\n\n\n\n\n\n\nFigure 5: data label\n\n\n\n\n\n\n\n\n\nFigure 6: What happened during training\n\n\n\nThus in Figure 6, during 1 individual training loop. Prediction vector is later will be compare with data label using loss function and error is compute.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#interpret-step-6",
    "href": "content/04_learning.html#interpret-step-6",
    "title": "About Learning I",
    "section": "",
    "text": "Bias : amount that a model’s prediction differs from the target value, compared to the training data.\nLow Bias: fewer strong assumptions or constraints placed during model training. In this case, the model will closely match the training dataset. Model has ability to capture the true underlying patterns or relationships present in the data\nHigh Bias : model simplifies the underlying patterns too much (strong assumptions or constraints placed during model training). In this case, the model will not match the training dataset closely. Usually called under-fitting\nVariance: the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\nLow Variance : not overly influenced by the noise or fluctuations in the training data. It tends to be more stable, providing consistent predictions even when trained on different subsets of the data.\nHigh Variance : overly sensitive to the noise or fluctuations in the training data. It tries to fit the training data so closely that it captures not only the true underlying patterns but also the random noise in the data. Usually called over-fitting\nHigh Bias and Low Variance: Consistent but inaccurate due to oversimplification. \\(\\Longrightarrow\\) poor performance\nHigh Variance and Low Bias: Accurate but inconsistent, capturing noise and over-fitting to the training data. \\(\\Longrightarrow\\) poor performance\n\n\n\n\n\n\n\nFigure 7: What happened during training\n\n\n\nOverfitting is when your model is basically memorising the data without really understanding/interpolating a general function that would apply to any external data. In other words it creates a function that is tailored specifically to the training dataset thus it would perform near perfectly with the training data, but it would perform quite badly with any data that it hasn’t seen before. (models the training data too well but fails to generalize to new data)\nUnderfitting is simply when you do not have enough data for the model to create/interpolate a general function that describes the process. This leads to the model performing badly for most input data. (model that fails to capture the underlying pattern in the training data)\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 8: Training trade-off4\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/02_eda.html",
    "href": "content/02_eda.html",
    "title": "Overview",
    "section": "",
    "text": "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform basic data exploration to study the basic characteristics of a dataset.\nBy studying it basic properties, we may find useful patterns (trend), connections, and relationships within data. This is usually called data exploration or exploratory data analysis (EDA).\nThe whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is good or not.\nOnce we have good data, we can training our machine learning model and get accurate results.\n\n\n\ndescriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot\n\n\n\n\n\n\n\n\nMany times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-analysis",
    "href": "content/02_eda.html#common-data-analysis",
    "title": "Overview",
    "section": "",
    "text": "descriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-pre-processing",
    "href": "content/02_eda.html#common-data-pre-processing",
    "title": "Overview",
    "section": "",
    "text": "Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/00_intro-ml-life-cyle.html",
    "href": "content/00_intro-ml-life-cyle.html",
    "title": "Machine Learning Life-Cycle",
    "section": "",
    "text": "Wisdom\n\n\n\n“The purpose of computing is insight, not numbers.” - Richard Hamming,mathematician\n\n\n\nMachine Learning Life-Cycle\nSince Machine Learning (ML) is a collection of learning techniques used to extract “valuable insight” from data, we need to know what to say about data-at-hand, how machine extract & learnt from data and how to evaluate “extraction” procedure.\n\n\n\n\n\n\nFigure 1: ML Lifecycle1\n\n\n\n\nStatistics syntax in Data Science\n\n\nStatistical Term\nData Science Term\n\n\n\n\nEstimation\nLearning\n\n\nClassification\nSupervised Learning\n\n\nClustering\nUnsupervised Learning\n\n\nCovariate/Independent variable\nFeatures\n\n\nClassifier\nHypothesis\n\n\n\n\n\n\n\n\nReferences\n\n1. Watson, A. Synthetic data and the data-centric machine learning life cycle. (2022).",
    "crumbs": [
      "Machine Learning Life Cycle"
    ]
  },
  {
    "objectID": "content/01_data_collection.html",
    "href": "content/01_data_collection.html",
    "title": "Introduction",
    "section": "",
    "text": "First we must differentiate between data at hand - which is the data that is available to us, and data not in hand, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to data collecting.\n\n\nBefore collecting data for your problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)\n\n\n\n\nNowadays, the availability of large volumes of data and the widespread use of tools for the proper extraction of knowledge information has become very frequent.\nMost learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem\n\n\n\n\n\n\n\nData sampling : in which the dataset instances are modified in such a way as to produce a more balanced class distribution.\nAlgorithmic modification\nCost-sensitive learning\n\n\n\n\n\n\n\n\n\nFigure 4: Downsample and Upsample\n\n\n\n\nUpsampling : process of randomly duplicating observations from the minority class to reinforce its signal.\nDownsampling: sample the majority class randomly to make their frequencies closer to the rarest class.\nhybrid: some methodologies do a little of both and possibly impute synthetic data for the minority class.\n\n\n\n\nInstead of focusing on modifying the training set in order to combat class skew, this approach aims at modifying the classifier learning procedure itself.\n\n\n\nCost-sensitive learning for imbalanced classification is focused on first assigning different costs to the types of misclassification errors that can be made, then using specialized methods to take those costs into account.\n\n\n\n\n\n\nFigure 5: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right2\n\n\n\n\n\n\n\n\n\n\n\n\n\nWisdom\n\n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n“no learner(classifier) can beat random guessing over all possible functions to be learned” - no free lunch theorem3\n\n\nThe fundamental goal of machine learning is to generalize beyond the examples in the training set. This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time.\nQuestion : To “learn” and achieve good generalization, how much data do we need?\n\n\n\n\nincrease the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 6: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.4\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Effect of training data size on accuracy on cifar-10 images\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Effect of training data size on accuracy on medical(CT scan image) classification\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Effect of training data size on accuracy on credit card fraud detection\n\n\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\n\n\n\n\n\nWisdom\n\n\n\nEntropy quantifies how much information there is in a random variable (recall variability)",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#goodness-of-dataset",
    "href": "content/01_data_collection.html#goodness-of-dataset",
    "title": "Introduction",
    "section": "",
    "text": "Before collecting data for your problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#general-problem",
    "href": "content/01_data_collection.html#general-problem",
    "title": "Introduction",
    "section": "",
    "text": "Nowadays, the availability of large volumes of data and the widespread use of tools for the proper extraction of knowledge information has become very frequent.\nMost learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#addressing-imbalanced-dataset",
    "href": "content/01_data_collection.html#addressing-imbalanced-dataset",
    "title": "Introduction",
    "section": "",
    "text": "Data sampling : in which the dataset instances are modified in such a way as to produce a more balanced class distribution.\nAlgorithmic modification\nCost-sensitive learning\n\n\n\n\n\n\n\n\n\nFigure 4: Downsample and Upsample\n\n\n\n\nUpsampling : process of randomly duplicating observations from the minority class to reinforce its signal.\nDownsampling: sample the majority class randomly to make their frequencies closer to the rarest class.\nhybrid: some methodologies do a little of both and possibly impute synthetic data for the minority class.\n\n\n\n\nInstead of focusing on modifying the training set in order to combat class skew, this approach aims at modifying the classifier learning procedure itself.\n\n\n\nCost-sensitive learning for imbalanced classification is focused on first assigning different costs to the types of misclassification errors that can be made, then using specialized methods to take those costs into account.\n\n\n\n\n\n\nFigure 5: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right2",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#how-much-data",
    "href": "content/01_data_collection.html#how-much-data",
    "title": "Introduction",
    "section": "",
    "text": "Wisdom\n\n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n“no learner(classifier) can beat random guessing over all possible functions to be learned” - no free lunch theorem3\n\n\nThe fundamental goal of machine learning is to generalize beyond the examples in the training set. This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time.\nQuestion : To “learn” and achieve good generalization, how much data do we need?",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#observed-and-test",
    "href": "content/01_data_collection.html#observed-and-test",
    "title": "Introduction",
    "section": "",
    "text": "increase the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 6: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.4\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Effect of training data size on accuracy on cifar-10 images\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Effect of training data size on accuracy on medical(CT scan image) classification\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Effect of training data size on accuracy on credit card fraud detection\n\n\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\n\n\n\n\n\nWisdom\n\n\n\nEntropy quantifies how much information there is in a random variable (recall variability)",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html",
    "href": "content/03_data_labelling.html",
    "title": "Overview",
    "section": "",
    "text": "In machine learning, data labeling, or data annotation is the process of identifying raw data (images, text files, videos, etc.) and adding one or more meaningful and informative labels to provide context so that a machine learning model can learn from it.\n\n\nFor classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html#computer-vision-and-audio",
    "href": "content/03_data_labelling.html#computer-vision-and-audio",
    "title": "Overview",
    "section": "",
    "text": "For classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/05_evaluation.html",
    "href": "content/05_evaluation.html",
    "title": "About Learning III",
    "section": "",
    "text": "At the end of “training” (after model optimization), we need to ask the following:\n\nHow well is our model doing?\nIs our model good enough for us to use?\n\nTo answer this, we must do model evaluation. And to evaluate, certain measurement or metric is used to judge (evaluate) the performance of your model. It provides a more interpretable measure of your model’s performance.\nAlso recall that our “learning” output answer will always be in terms of probability.\nThe end goal is to:\n\nOptimal: our trained model(s) performs as well in unseen dataset\nReliable: our trained model(s) behaves as expected\n\n\n\nHere are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class, and the actual value was also positive\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class, and the actual value was also negative\nFalse Positives (FP) is an outcome where the model predicted the positive class incorrectly, and the actual value was negative\nFalse Negatives (FN) is an outcome where the model predicted the negative class incorrectly, and the actual value was positive .\n\n\n\n\n\n\n\nFigure 1: common general metric\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Example 1\n\n\n\n\n\n\n\n\n\nFigure 3: Example 2\n\n\n\n\n\n\n\n\n\nFigure 4: Example 3\n\n\n\nPrecision (or correctness): This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall (sensitivity): a measure of how actual observations are predicted correctly.This shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\nF1-score: is the harmonic mean of the precision and recall values\n\\[\nF1  Score = \\frac{2 \\times Precision \\times  Recall}{Precision +  Recall}\n\\]",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/05_evaluation.html#common-metrics",
    "href": "content/05_evaluation.html#common-metrics",
    "title": "About Learning III",
    "section": "",
    "text": "Here are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class, and the actual value was also positive\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class, and the actual value was also negative\nFalse Positives (FP) is an outcome where the model predicted the positive class incorrectly, and the actual value was negative\nFalse Negatives (FN) is an outcome where the model predicted the negative class incorrectly, and the actual value was positive .\n\n\n\n\n\n\n\nFigure 1: common general metric",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/05_evaluation.html#example",
    "href": "content/05_evaluation.html#example",
    "title": "About Learning III",
    "section": "",
    "text": "Figure 2: Example 1\n\n\n\n\n\n\n\n\n\nFigure 3: Example 2\n\n\n\n\n\n\n\n\n\nFigure 4: Example 3\n\n\n\nPrecision (or correctness): This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall (sensitivity): a measure of how actual observations are predicted correctly.This shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\nF1-score: is the harmonic mean of the precision and recall values\n\\[\nF1  Score = \\frac{2 \\times Precision \\times  Recall}{Precision +  Recall}\n\\]",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/07_deployment.html",
    "href": "content/07_deployment.html",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "When a data scientist has a model ready, the next step is to deploy it in a way that it can serve the application.\nThe basic meaning of model serving is to host machine-learning models (on the cloud or on premises) and to make their functions available via API so that applications can incorporate AI into their systems.1\nDeploying a machine-learning model in production also involves resource management and model monitoring including operations stats as well as model drifts.\nThings to consider:\n\nAccess points (endpoints): An endpoint is a URL that allows applications to communicate with the target service via HTTPS protocol\nTraffic management: Requests at an endpoint go through various routes, depending on the destination service. Traffic management may also deploy a load-balancing feature to process requests concurrently.\nPre- and post-processing requests: A service may need to transform request messages into the format suitable for the target model and convert response messages into the format required by client applications. Often, serverless functions can handle such transformations.\nMonitor model drifts: We must monitor how each machine-learning model performs and detect when the performance deteriorates and requires retraining.\n\n\n\nServing the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!\n\n\n\nModel monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.\n\n\n\n\n\nA conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#model-serving-strategy",
    "href": "content/07_deployment.html#model-serving-strategy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Serving the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#model-monitor",
    "href": "content/07_deployment.html#model-monitor",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Model monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#privacy",
    "href": "content/07_deployment.html#privacy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "A conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/layout_planing.html",
    "href": "content/layout_planing.html",
    "title": "Draft Planning",
    "section": "",
    "text": "What is not in scope\n\n\n\n\nThis is not a math course or ML Theory course\nThis is not programming/python course\nThis is not an ML system development course\n\n\n\n\n\n\n\n\n\nLearning objective\n\n\n\n\n\nTo design effective ML solution, by …\n\npreparing suitable data for modeling\nto build generalizable model\noptimization decision system\n\n\n\n\n\n\n\ndata scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician ?\na data scientist is someone who extracts insights from messy data. Today’s world is full of people trying to turn data into insight\n\n\n\nInference\n\nbasic of probability theory\nhypothesis testing\nsignificance test\nrandomization inference\npower and confidence\nbayesian inference\n\nMachine Learning\n\nregression\nregularization\nrecommender system\nclassification\nunsupervised learning\nneural networks\n\n\n\n\n\n\nhttps://github.com/briandalessandro/DataScienceCourse/tree/master/ipython/Lectures\nhttps://nyu-ds1003.github.io/spring2023/#lectures\nhttps://github.com/memosys/NYU-DS-MS-AT-HOME"
  },
  {
    "objectID": "content/layout_planing.html#what-is-data-science",
    "href": "content/layout_planing.html#what-is-data-science",
    "title": "Draft Planning",
    "section": "",
    "text": "data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician ?\na data scientist is someone who extracts insights from messy data. Today’s world is full of people trying to turn data into insight\n\n\n\nInference\n\nbasic of probability theory\nhypothesis testing\nsignificance test\nrandomization inference\npower and confidence\nbayesian inference\n\nMachine Learning\n\nregression\nregularization\nrecommender system\nclassification\nunsupervised learning\nneural networks"
  },
  {
    "objectID": "content/layout_planing.html#ref",
    "href": "content/layout_planing.html#ref",
    "title": "Draft Planning",
    "section": "",
    "text": "https://github.com/briandalessandro/DataScienceCourse/tree/master/ipython/Lectures\nhttps://nyu-ds1003.github.io/spring2023/#lectures\nhttps://github.com/memosys/NYU-DS-MS-AT-HOME"
  },
  {
    "objectID": "content/use_case.html",
    "href": "content/use_case.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "So far we have learnt"
  },
  {
    "objectID": "content/use_case.html#model-trade-off",
    "href": "content/use_case.html#model-trade-off",
    "title": "Supervised Learning",
    "section": "Model Trade-Off",
    "text": "Model Trade-Off\n\n\n\n\n\n\nFigure 2: Trade-off between model interpretability and performance\n\n\n\nWhen building a model, things to consider:"
  },
  {
    "objectID": "content/use_case.html#use-case",
    "href": "content/use_case.html#use-case",
    "title": "Supervised Learning",
    "section": "Use Case",
    "text": "Use Case\n\nCase-1: Face Recognition/Verification\n\n\nCase-2: Weather(Rain) Prediction\n\n\nCase-3: Housing Price Prediction\n\nList of possible question (theory):\n\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n\nThe sample size n is extremely large, and the number of predictors p is small.\nThe number of predictors p is extremely large, and the number of observations n is small.\nThe relationship between the predictors and response is highly non-linear.\nThe variance of the error terms, i.e. \\(\\sigma^2\\) = Var\\((\\epsilon)\\), is extremely high.\n\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n\nWe collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price,and ten other variables.\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\nWe now revisit the bias-variance decomposition.\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.\nExplain why each of the five curves has the shape displayed in part (a).\n\nYou will now think of some real-life applications for statistical learning.\n\nDescribe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\nDescribe three real-life applications in which cluster analysis might be useful.\n\nWhat are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?\nDescribe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?\nSuppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nWhat is a feature space? What do the dimensions of such a space represent? What is a vector? What is a feature vector?\n\n9.If we want to use the values of F different features, in order to classify objects, where each feature can have any of G different values, what is the dimension of the feature space?\n\nFor a 12 × 12 grayscale image (256 grayscale levels), how many dimensions are there for the feature vector? How many different possible feature vectors are there?\nTrue or false: A probability density can take on values greater than 1.\nTrue or false: A probability density can take on values less than 0.\nWhat is a decision rule? If there are 10 possible feature vectors, how many possible decision rules are there?\nWhat are the advantages of choosing/using more features and what are the disadvantages?\nWhat is the general learning problem we are concerned with?\nWhy do we need learning? Why cannot we simply use a Bayes rule for pattern classification?\nWhat are training data for learning from examples?\nWhat sorts of assumptions do we need to make about the training data?\nWhat is the curse of dimensionality?\nWhat is inductive bias? Is it good or bad?\na(a) Sketch a diagram of a perceptron with three inputs x1, x2, x3 and weights w1,w2,w3. Label the inputs, weights, and output.\n\n2 consider a classification problem in which each instance consists of \\(d\\) features \\(x1, . . . , xd\\), each of which can take on only the values 0 or 1. A feature vector belongs to class 0 if \\(x_1 + x_2 +· · ·+x_d\\) is even (i.e., the number of 1’s is even) and it belongs to class 1 otherwise. Can this problem be solved by a single perceptron? A three-layered network? Why or why not?\n\nIf a loss function other than squared error is used, will the regression function (i.e., the conditional mean of y, given \\(\\bar{x}\\)) still always be the best estimator?\nwith less than 10 words, what is the purpose of machine learning?\nImagine you are solving a classification problem with a highly imbalanced class.\nThe majority class is observed 99% of the time in the training data. Your model has 99% accuracy after taking the predictions on the test set. Which of the following is true in such a case?\n   1.The accuracy metric is not a good idea for imbalanced class problems.\n   2.The accuracy metric is a good idea for imbalanced class problems.\n   3.Precision and recall metrics are good for imbalanced class problems.\n   4.Precision and recall metrics aren’t good for imbalanced class problems.\n\n1 and 3\n1 and 4\n2 and 3\n2 and 4\n\nSolution: (A)\nIn machine learning, what is the primary difference between supervised and unsupervised learning?\n1.Supervised learning involves data that has been labeled and classified, while unsupervised learning data is unlabeled and unclassified. 2.Supervised learning is monitored closely by data scientists, while they don’t play a role in unsupervised learning. 3.Supervised learning is only used for image recognition, while unsupervised learning can be used for various analytics applications.\nTrue or false? Bias is a common problem in data science applications.\nWhy is data sampling useful for data scientists?\n\nIt lets them analyze data sets in small batches to reduce their use of system resources.\nIt reduces the amount of data storage space that’s required for data science applications.\nIt enables them to use a representative subset of data to build accurate analytical models more quickly.\n\nIn traditional computer programming, you input commands. What do you input with machine learning?\n\npatterns\nprograms\nrules\ndata\n\nWhy is it important for machine learning algorithms to have access to high-quality data?\nIt will take too long for programmers to scrub poor data. If the data is high quality, the algorithms will be easier to develop. Low-quality data requires much more processing power than high-quality data. If the data is low quality, you will get inaccurate results.[aws]\nYour company wants to predict whether existing automotive insurance customers are more likely to buy homeowners insurance. It created a model to better predict the best customers contact about homeowners insurance, and the model had a low variance but high bias. What does that say about the data model?\n1.It was consistently wrong.(ini jwABAN) 2.It was inconsistently wrong.\n\nIt was consistently right.\nIt was equally right end wrong.\n\nYou want to identify global weather patterns that may have been affected by climate change. To do so, you want to use machine learning algorithms to find patterns that would otherwise be imperceptible to a human meteorologist. What is the place to start?\n\nFind labeled data of sunny days so that the machine will learn to identify bad weather.\nUse unsupervised learning have the machine look for anomalies in a massive weather database.\nCreate a training set of unusual patterns and ask the machine learning algorithms to classify them.\nCreate a training set of normal weather and have the machine look for similar patterns. [anw]\n\nYou work for a power company that owns hundreds of thousands of electric meters. These meters are connected to the internet and transmit energy usage data in real-time. Your supervisor asks you to direct project to use machine learning to analyze this usage data. Why are machine learning algorithms ideal in this scenario?\nThe algorithms would help the meters access the internet. The algorithms will improve the wireless connectivity. The algorithms would help your organization see patterns of the data.[anw] By using machine learning algorithms, you are creating an IoT device.\nHow is machine learning related to artificial intelligence?\nArtificial intelligence focuses on classification, while machine learning is about clustering data. Machine learning is a type of artificial intelligence that relies on learning through data. Artificial intelligence is form of unsupervised machine learning. Machine learning and artificial intelligence are the same thing.\nWhat is one reason not to use the same data for both your training set and your testing set?\nYou will almost certainly underfit the model. You will pick the wrong algorithm. You might not have enough data for both. You will almost certainly overfit the model.[anw]\nAre data model bias and variance a challenge with unsupervised learning?\nNo, data model bias and variance are only a challenge with reinforcement learning. Yes, data model bias is a challenge when the machine creates clusters.[anw] Yes, data model variance trains the unsupervised machine learning algorithm. No, data model bias and variance involve supervised learning.\nMany of the advances in machine learning have come from improved ___.\nstatistics structured data availability algorithms[anw]\nYou work for a website that enables customers see all images of themselves on the internet by uploading one self-photo. Your data model uses 5 characteristics to match people to their foto: color, eye, gender, eyeglasses and facial hair. Your customers have been complaining that get tens of thousands of photos without them. What is the problem?\nYou are overfitting the model to the data You need a smaller training set You are underfitting the model to the data[anw] You need a larger training set\n___ refers to a model that can neither model the training data nor generalize to new data.\ngood fitting overfitting underfitting[anw] all of the above\nWhat does it mean to underfit your data model?\nThere is too little data in your training set. There is too much data in your training set. There is not a lot of variance but there is a high bias.[anw] Your model has low bias but high variance.\nExplanation: Underfitted data models usually have high bias and low variance. Overfitted data models have low bias and high variance.\nAsian user complains that your company’s facial recognition model does not properly identify their facial expressions. What should you do?\nInclude Asian faces in your test data and retrain your model. Retrain your model with updated hyperparameter values. Retrain your model with smaller batch sizes. Include Asian faces in your training data and retrain your model.[anw]\nThe new dataset you have just scraped seems to exhibit lots of missing values. What action will help you minimizing that problem?\nWise fill-in of controlled random values Replace missing values with averaging across all samples Remove defective samples Imputation[anw]\nWhich choice is the best example of labeled data?\na spreadsheet [anw] 20,000 recorded voicemail messages 100,000 images of automobiles hundreds of gigabytes of audio files\nThe data in your model has low bias and low variance. How would you expect the data points to be grouped together on the diagram?\nThey would be grouped tightly together in the predicted outcome.[anw] They would be grouped tightly together but far from the predicted. They would be scattered around the predict outcome. They would be scattered far away from the predicted outcome.\nIn supervised machine learning, data scientist often have the challenge of balancing between underfitting or overfitting their data model. They often have to adjust the training set to make better predictions. What is this balance called?\nthe under/over challenge balance between clustering classification bias-variance trade-off[anw] the multiclass training set challenge\nIf you are thinking about using machine learning algorithms, the best thing you can do today is to ensure you have quality _.\ndata[anw] processors networking statistical techniques\nYou’ve received 1,000,000 images and have split it in 96%/2%/2% between train, dev and test sets. You’ve trained your model, and analyzed the results. After working further on the problem, you’ve decided to correct the incorrectly labeled data on the dev set.\n\nWhich of these statements do you agree with?\nYou should also correct the incorrectly labeled data in the test set, so that the dev and test sets still come from the same distribution.[anw]\nYou should correct incorrectly labeled data in the training set as well so as to avoid your training set now being even more different from your dev set.\nYou should not correct the incorrectly labeled data in the test set, because the test set should reflect the data distribution of the real world.\nIf you want to correct incorrectly labeled data, you should do it on all three sets (train/dev/test) in order to maintain similar distributions.\n\nIn simple words, Define precision and recall.\nAs the number of training examples goes to infinity, your model trained on that data will have:\nA. Lower variance [anw]\nB. Higher variance \nC. Same variance\n\nExplanation: It is important that your dev and test set have the closest possible distribution to “real” data.\n\nDuring the data preprocessing step, how should one treat missing/null values? How will you deal with them?\nWhich approach is used for handling imbalanced datasets in classification tasks, where one class has significantly fewer samples than the others?\n\nOverfitting\nData Augmentation\nOversampling the minority class\nFeature Scaling\n\nAnswer: c) Oversampling the minority class"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]