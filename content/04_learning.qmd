# About Learning I

## Big Picture

To automatically discover regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

or the ability to automatically learn from data and past experiences while identifying patterns to make predictions with minimal human intervention.


## Learning Overview

![Different learning strategy](image/jenis_ml.png){#fig-learning-strategy}

## The "problem setting" of statistical learning.

The basic setting of statistical learning is: given a **problem statement**, we want to find **prediction model** which **estimate** has **best fit** in providing **solutions** to the problem, using **data at hand** (in sample data) which has the **Lowest Variance and Lowest Bias** when applied to the data at hand as well as to the **data not in hand** (unseen data).



## Statistical Modelling [@hasni]

From a statistical learning point-of-view:

$$
Y = f(\bf{X}) + \epsilon
$$ where ${X}$ is ***input*** variable:

$$
X = (X_1,X_2,...,X_p)
$$ and $\epsilon$ is ***noise***.

The task is to get prediction/estimation of:

$$
\hat{Y} = \hat{f}(\bf{X}) + \hat{\epsilon}
$$ The task is then relegated to *error estimator*, by defining **Loss function**:

$$ 
Error(x) = E[(Yâˆ’\hat{f}(\bf{X}))^2]
$$

![Visualize learning model](image/basic_model.png){#fig-learning-model}

A loss function, also known as a cost function, is a method used to estimate the discrepancies between the actual and predicted values in a machine learning model. It provides a measure of how well the model is performing.

noise in data refers to unwanted modifications introduced to a source signal during the capture, storage, transmission, or processing of its information.

## **How to Learn ?**

### Data representation

![Example 1:Image data representation in vector](image/image_rep.png){#fig-img-rep}

But wait, how about textual data ?

![Example 3:Text data representation in vector](image/text_rep.png){#fig-text-rep}

### The "learning"

Imagine you have a set of data. In order to have ***good estimated*** model, we have to split data at hand into:

-   Training set: This is the largest part in terms of the size of the dataset.

-   Validation set: model training process is not a one-time process (highly iterative process). We have to train multiple models by trying different combinations of parameters (complexity). Then, we evaluate the performance of each model on the validation set.

-   Test set: this is use ***after the training*** to evaluate performance of the model via *un-seen* data

![generic learning process](image/learning.png){#fig-learn}

------------------------------------------------------------------------

-   What do we want? $\Longrightarrow$ To make *predictions* on *unseen data* $\Longrightarrow$ We want a *model* that **generalizes** well $\Longrightarrow$ generalizes to unseen data
-   How we will do this? $\Longrightarrow$ controlling the **complexity** of the model (learning parameter)
-   How do we know if our model generalizes? $\Longrightarrow$ evaluating on **test** data.

![Training trade-off [@javatpoint]](image/biasvariance.png){#fig-model-tradeoff}


> Learning is **NOT** memorization! The ability to produce correct outputs on previously unseen inputs is called **generalization**