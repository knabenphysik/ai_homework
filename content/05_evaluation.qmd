# About Learning III

At the end of "training" (after model optimization), we need to ask the following:

1.  How well is our model doing?

2.  Is our model good enough for us to use?

To answer this, we must do **model evaluation**. And to evaluate, certain measurement or metric is used to judge (evaluate) the performance of your model. It provides a more interpretable measure of your model's performance.

Also recall that our "learning" output answer will always be in terms of probability.

## Common metrics

Here are the essential metrics:

- **True Positives (TP)** is an outcome where the model correctly predicts the positive class. 

- **True Negatives (TN)** is an outcome where the model correctly predicts the negative class

- **False Positives (FP)** is an outcome where the model incorrectly predicts the positive class.

- **False Negatives (FN)** is an outcome where the model incorrectly predicts the negative class.

![common general metric](image/metrics_common.png){#fig-common-metrics} 

**Precision**: This metric shows how often your model is **correct** when predicting the **target class**.

$$
Precision = \frac{TP}{TP + FP}
$$
**Recall**: metric that shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)

$$
Recall = \frac{TP}{TP + FN}
$$

- High precision and high recall mean that your model is performing well. 
- Low precision means that your model will predict some false positives
- Low recall means that your model will predict some false negatives
