{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# About Data\n",
        "\n",
        "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform *`basic data exploration`* to study the basic characteristics of a dataset.\n",
        "\n",
        "By studying it basic properties, we may find useful patterns, connections, and relationships within data. This is usually called *`data exploration`* or *`exploratory data analysis (EDA)`*.\n",
        "\n",
        "The whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is ***good or not***.\n",
        "\n",
        "Once we have ***good data***, we can training our machine learning model and get ***accurate*** results.\n",
        "\n",
        "Common type of dataset is:\n",
        "\n",
        "-   tabular/numerical data\n",
        "-   categorical data (image, voice, videos)\n",
        "-   time series\n",
        "-   text\n",
        "\n",
        "A goodness of dataset:\n",
        "\n",
        "-   quality\n",
        "-   quantity\n",
        "-   variability\n",
        "\n",
        "![common python libraries package for ML](image/ml_lib.png){#fig-ml-lib}\n",
        "![cloud service for ML ](image/ml_cloud.png){#fig-ml-cloud}\n",
        "\n",
        "## Tabular Data\n",
        "\n",
        "-   scatter plot $\\Longrightarrow$ two variables are plotted along two axes.\n",
        "-   pairplot $\\Longrightarrow$ pairwise relationships between variables within a dataset\n",
        "\n",
        "The closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\n",
        "\n",
        "If a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: **positive** or **negative**, **strong** or **weak**.\n",
        "\n",
        "![correlation plot](image/corre.png){#fig-corre}\n",
        "\n",
        "So, in statistical terms we use correlation to denote association between two quantitative variables.\n",
        "\n",
        "### Example 1\n",
        "\n",
        "Imagine you have tabular [data](https://www.kaggle.com/competitions/titanic/data) as below.\n"
      ],
      "id": "6847bd85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-dummy-data\n",
        "#| tbl-cap: Dummy Dataset\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data1 = pd.read_csv(\"data/dummy1.csv\")\n",
        "data1.head()"
      ],
      "id": "tbl-dummy-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset in @tbl-dummy-data have 7 columns $\\Longrightarrow$ 7 features\n",
        "\n",
        "Question : From @fig-corre, what is the best way to describe or visualize the data given to us? Answer: Let's do pair-plot (combination of scatter plot)\n"
      ],
      "id": "166024c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 5,
        "fig-height": 4
      },
      "source": [
        "#| label: fig-pair-plot\n",
        "#| fig-cap: pair plot\n",
        "#| echo: false\n",
        "#| column: screen\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mplhep as hep\n",
        "import seaborn as sns\n",
        "hep.style.use(\"CMS\")\n",
        "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
        "plt.rcParams.update({'font.size': 7})\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "colors = iter(['xkcd:red purple', 'xkcd:pale teal', 'xkcd:warm purple',\n",
        "       'xkcd:light forest green', 'xkcd:blue with a hint of purple',\n",
        "       'xkcd:light peach', 'xkcd:dusky purple', 'xkcd:pale mauve',\n",
        "       'xkcd:bright sky blue', 'xkcd:baby poop green', 'xkcd:brownish',\n",
        "       'xkcd:moss green', 'xkcd:deep blue', 'xkcd:melon',\n",
        "       'xkcd:faded green', 'xkcd:cyan', 'xkcd:brown green',\n",
        "       'xkcd:purple blue', 'xkcd:baby shit green', 'xkcd:greyish blue'])\n",
        "\n",
        "def my_scatter(x,y, **kwargs):\n",
        "    kwargs['color'] = next(colors)\n",
        "    plt.scatter(x,y, **kwargs)\n",
        "\n",
        "def my_hist(x, **kwargs):\n",
        "    kwargs['color'] = next(colors)\n",
        "    plt.hist(x, **kwargs)\n",
        "\n",
        "g = sns.PairGrid(data1, corner=True)\n",
        "g.map_diag(my_hist)\n",
        "g.map_offdiag(my_scatter)"
      ],
      "id": "fig-pair-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What can we say about @fig-pair-plot ?\n"
      ],
      "id": "2b3882e3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 5,
        "fig-height": 4
      },
      "source": [
        "#| label: fig-big-pair-plot\n",
        "#| fig-cap: big pair plot\n",
        "#| echo: false\n",
        "#| column: screen-inset-shaded\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mplhep as hep\n",
        "import seaborn as sns\n",
        "hep.style.use(\"CMS\")\n",
        "plt.rcParams['savefig.facecolor'] = \"0.8\"\n",
        "plt.rcParams.update({'font.size': 7})\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data2 = pd.read_csv(\"data/dummy2.csv\", usecols=lambda x: x != 'feature_A')\n",
        "sns.pairplot(data2, corner=True);"
      ],
      "id": "fig-big-pair-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2\n"
      ],
      "id": "46b2feeb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-big-dummy-data\n",
        "#| tbl-cap: Dummy Big Dataset\n",
        "#| echo: false\n",
        "\n",
        "data2.head()"
      ],
      "id": "tbl-big-dummy-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But if we have many features like @tbl-big-dummy-data and want to plot pair-plot like @fig-big-pair-plot, seem to overwhelming and confuse isn't?\n",
        "\n",
        "Solution? Use correlation heatmap $\\Longrightarrow$ easier to see based on correlation value/coefficient, r (recall our @fig-corre).r value is the degree of association.\n"
      ],
      "id": "38f4adeb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-correlation\n",
        "#| tbl-cap: correlation guide\n",
        "#| echo: false\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from tabulate import tabulate\n",
        "table = [[\"r < 0.3\", \"None or very weak\"],\n",
        "         [\"0.3 < r <0.5\", \"Weak\"],\n",
        "         [\"0.5 < r < 0.7\", \"Moderate\"],\n",
        "         [\"r > 0.7\", \"Strong\"]]\n",
        "Markdown(tabulate(\n",
        "  table, \n",
        "  headers=[\"Correlation Value (r)\", \"Strength of Relationship\"]\n",
        "))"
      ],
      "id": "tbl-correlation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, based on @tbl-correlation, let change our @fig-big-pair-plot to correlation heatmap\n"
      ],
      "id": "28665eaf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-big-correlation-plot\n",
        "#| fig-cap: correlation plot for big data\n",
        "#| echo: false\n",
        "\n",
        "data2 = data2.replace(['rain'],['0'])\n",
        "data2 = data2.replace(['Rain, Partially cloudy'],['0'])\n",
        "data2 = data2.replace(['Partially cloudy'],['1'])\n",
        "data2 = data2.replace(['Rain, Overcast'],['2'])\n",
        "data2 = data2.replace(['Overcast'],['3'])\n",
        "data2 = data2.replace(['partly-cloudy-night'],['4'])\n",
        "data2 = data2.replace(['cloudy'],['5'])\n",
        "data2 = data2.replace(['Rain'],['6'])\n",
        "data2 = data2.replace(['Clear'],['7'])\n",
        "data2 = data2.replace(['partly-cloudy-day'],['8'])\n",
        "data2 = data2.replace(['clear-night'],['9'])\n",
        "data2 = data2.replace(['clear-day'],['10'])\n",
        "\n",
        "corr = data2.corr(method='spearman')\n",
        "\n",
        "f,ax = plt.subplots(figsize=(9,6))\n",
        "sns.heatmap(corr, annot = True, fmt='.2g',cmap= 'coolwarm', ax=ax)\n",
        "# plt.subplot(1, 2, 1)\n",
        "ax.set_title ('Correlation Matrix for big data', fontdict = {'fontsize':10}, pad = 10);\n",
        "plt.show()"
      ],
      "id": "fig-big-correlation-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-small-correlation-plot\n",
        "#| fig-cap: correlation plot for @fig-pair-plot\n",
        "#| echo: false\n",
        "\n",
        "corr_ = data1.corr(method='spearman')\n",
        "\n",
        "f,ax = plt.subplots(figsize=(9,6))\n",
        "sns.heatmap(corr_, annot = True, fmt='.2g',cmap= 'coolwarm', ax=ax)\n",
        "# plt.subplot(1, 2, 1)\n",
        "ax.set_title ('Correlation Matrix for small data', fontdict = {'fontsize':10}, pad = 10);\n",
        "plt.show()"
      ],
      "id": "fig-small-correlation-plot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Data\n",
        "\n",
        "### Example 1\n",
        "\n",
        "Imagine you have mnist image [data](https://www.kaggle.com/competitions/digit-recognizer/data) below.\n"
      ],
      "id": "e99833e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mnist\n",
        "#| fig-cap: sample images of mnist dataset\n",
        "#| echo: false\n",
        "\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "fig, ax = plt.subplots(8, 8, figsize=(6,6))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(digits.images[i], cmap='binary')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "plt.show()"
      ],
      "id": "fig-mnist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, since our data now is images and not tabular. We can use simple ***histogram plot*** like @fig-mnist-count to view each ***class*** of MNIST dataset.\n",
        "\n",
        "-   We can observe that data distribution `almost` the same (almost balanced) for each class\n"
      ],
      "id": "3a037a79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-mnist-count\n",
        "#| fig-cap: images distribution of mnist dataset\n",
        "#| column: screen-inset-shaded\n",
        "#| echo: false\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "numberMNIST = fetch_openml('mnist_784',return_X_y=False)\n",
        "dataset = numberMNIST.data     \n",
        "labels = numberMNIST.target    \n",
        "\n",
        "X_train, X_test, Y_train, Y_test = dataset[:60000], dataset[60000:], labels[:60000], labels[60000:]\n",
        "\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(unique, counts);\n",
        "plt.xticks(unique);\n",
        "plt.xlabel(\"Label\");\n",
        "plt.ylabel(\"Quantity\");\n",
        "plt.title(\"Labels in MNIST 784 dataset\");"
      ],
      "id": "fig-mnist-count",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix\n",
        "\n",
        "We want our machine model to generalize as good as possible in order to evade overfitting, and thus many models assume the data to be ***normally distributed*** due to the [central limit theorem](https://theanalyticsgeek.com/central-limit-theorem/)\n",
        "\n",
        "See [here](https://www.kaggle.com/code/allunia/titanic-dive-through-feature-scaling-and-outliers) on example to normalize data."
      ],
      "id": "a43cd799"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}