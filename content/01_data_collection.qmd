# Introduction

First we must differentiate between __data at hand__ - which is the data that is available to us as data scientist, and __data not in hand__, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to __data collecting__.


## General Problem

Usually, in practice we have the following problem when we have __data at hand__.

1. small datasets
2. Long tail problem (organization data are specific) [@andrewGTC]

![the need of customization of specific AI projects](image/long_tail.png){#fig-longtail}

Thus, need to __*acquire*__ more!

## Data and sampling

Consider that you have big __dataset at hand__. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?

__Answer__: Yes! We have statistical approach which we called “Sampling”.

![Sampling from big data](image/data_sampling.png){#fig-data-sampling}

Type of sampling:

- __*random sampling*__ : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.
- __*cluster sampling*__:  we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected
- __*systematic sampling*__:  chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’
- __*stratified random sampling*__: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.


## Goodness of dataset

What makes a good dataset?

-   quality (representative and high-quality of inputs data)
-   quantity (consistent and accurate labels on target data/ground truth)
-   variability (reflect post deployment changes)


## How much data?

To **"learn"** and achieve **good generalization**, how much data do we need?

![Effect of training data size on accuracy on medical images](image/medical_plot2.png){#fig-img-cla}

@fig-img-cla showed the effect of training data size on [medical classification](https://arxiv.org/pdf/1511.06348.pdf) problem. From that report (see learning curve), learning classifier needs a training data set per class of 4092 to reach the desired accuracy, 99.5%.

![Effect of training data size on accuracy on cifar-10 images](image/cifar10_plot.png){#fig-multi-cla}

In contrast, @fig-multi-cla showed the effect of training data size on [cifar-10 images](https://ieeexplore.ieee.org/abstract/document/8599448) problem. From that report (see learning curve), learning classifier needs a training data set per class of 6000 to reach the desired accuracy of above 90%.

![Effect of categories and instances size for YoloV5](image/yolov5.png){#fig-yolov5}

And for @fig-yolov5 is the famous Yolo object detection classifier where number of images per category is recommended around ≥ 1500.

> "To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case." --- Maksym Tatariants

> For instance, annotating segmentation masks for a driving data set takes between 15
to 40 seconds per object. For 100,000 images the annotation could require between 170 and 460
days-equivalent of time.

We always prefer **large amount** of data, but how large is large, and how big is big? This is a problem of **sufficiency**, because even though the data may be large, but contains insufficient **entropy**, will render the data to be small, despite the large size in bytes.


## Dealing with data
### Data transformation

Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.

### Data augmentation

Data augmentation is used when we want to add “organized entropy” into an existing data set. This is only meaningful if those entropy helps to expand the data at hand, without altering the “meaningful aspects” of the data. That’s why it is useful in image processing or voice, which structures are not altered, but increase the space and dimensions.

### Data de-noising

Opposite to augmentation is “de-noising”, where we apply filters to take out the noises in the data. The argument here is reverse that is to reduce entropy in the data. Again, this is meaningful if there are no alterations to the basic structure of the data; and hence useful in image or voice processing. In NLP, removal of stop-words is a de-noising exercise.

### Data pre-processing

Data pre-processing may involve all of the above: transformation, augmentation and de-noising. In some cases all are required and helpful, in some cases a mixture of them will do. The basic process however is always data transformation.

### Dimensionality  Reduction

Dimensionality Reduction is a method of mapping a set of data onto a smaller space, represented by unique mapping between the raw data and a vector space, which serves as a “look-up table”. This reduction does not alter the structure of the data, instead it just compressed the data into a smaller space in terms of computer memories. Instead of working with raw data, we deal with its "essence" representations. An example of this is tokenization in NLP.

### Missing-data imputation

Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.

### Data Duplicate

To reduce model biased toward certain patterns in data, data duplicate should be remove.

## Dealing with Imbalanced data

Imbalanced classes are a common problem in machine learning where there are a disproportionate ratio of observations in each data category.

Some popular techniques:

- __*Oversampling Minority Category*__: Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don't have a ton of data to work with. A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.

- __*Undersampling Majority Class*__: Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.

- __*Generate Synthetic Samples*__: SMOTE or Synthetic Minority Oversampling Technique is a popular algorithm to creates synthetic observations of the minority class.