# Introduction

First we must differentiate between __data at hand__ - which is the data that is available to us as data scientist, and __data not in hand__, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to __data collecting__.

## Goodness of dataset

Before collecting data for your machine learning problem, we first need to know __"what makes a good dataset"__?

-   quality (representative and high-quality of inputs data)
-   quantity (consistent and accurate labels on target data/ground truth)
-   variability (reflect post deployment changes)


## General Problem

Most learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.

However, in real-world applications, training samples( __data at hand__ ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples [@zhang2023deep].

![Long Tail Data Illustration](image/long_tail_2.png){#fig-longtail_normal}

Long-tail data is visually represented by a hyperbolic curve like in @fig-longtail_normal.

So long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.


Consider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather. 

The point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. @fig-tesla is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.

![long tail scenario](image/car_sight.png){#fig-tesla}

Thus, need to __*acquire*__ more of these rare cases in our training data!

![class imbalance problem](image/long_tail_1.png){#fig-solve-long}


## Data and sampling

Consider that you have big __dataset at hand__. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?

__Answer__: Yes! We have statistical approach which we called “Sampling”.

![Sampling from big data](image/data_sampling.png){#fig-data-sampling}

Type of sampling:

- __*random sampling*__ : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.
- __*cluster sampling*__:  we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected
- __*systematic sampling*__:  chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’
- __*stratified random sampling*__: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.


## How much data?

To **"learn"** and achieve **good generalization**, how much data do we need?

- increasing the dataset sample size is a reduction in model over-fitting (avoid the model "memorize")
- be careful of noise, outliers, and irrelevant information in additional data (recall "Goodness of dataset")


![Effect of training data size on accuracy on cifar-10 images](image/cifar10_plot.png){#fig-multi-cla}

@fig-multi-cla showed the effect of training data size on [cifar-10 dataset](https://ieeexplore.ieee.org/abstract/document/8599448). From that report (see learning curve), learning classifier needs a training data set per class of 6000 to reach the desired accuracy of above 90%.

@fig-multi-pascal also show similar insight showing the learning classifier will improve its accuracy as the dataset is increase.

![Effect of training data size on accuracy on PASCAL [@Zhu_2015] Dataset](image/data_size_plot.png){#fig-multi-pascal} 

> "To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case." --- Maksym Tatariants


We always prefer **large amount** of data, but how large is large, and how big is big? This is a problem of **sufficiency**, because even though the data may be large, but contains insufficient **entropy**, will render the data to be small, despite the large size in bytes.

>  **Entropy** quantifies how much information there is in a random variable, or more specifically its probability distribution.

## Dealing with data

Now, we already establish criteria of good data & how much data to collect.

Assume now you already collect data and still facing __"imbalanced"__ where a small fraction of categories have a massive number of samples, and the rest of the categories are associated with only a few samples. What shall we do ?


![ Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right [@Johnson]](image/imbalanced.png){#fig-imbalanced}


### Class/Category Re-balancing

via "data method":

- pre-processing
  - __*Oversampling Minority Category*__: Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don't have a ton of data to work with. A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.
  
  - __*Undersampling Majority Class*__: Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.

- post-processing
  - __threshold__ :  Changing the decision threshold using prior class probabilities
  

via Algorithm-level methods:

- __loss function__ 

> A loss function is a mathematical function that quantifies the difference between predicted and actual values in a machine learning model. It measures the model's performance and guides the optimization process by providing feedback on how well it fits the data.

- __transfer learning__ 

- __2-stage learning__

- __deep ensemble__