[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "AI is everywhere!\nSome notable examples:\n\ngoogle lens (region proposal network)\ngoogle translate (Transformer-based)\nYouTube automatic captioning (automatic speech recognition)\nGmail spam filters (rule-based filters + Density clustering)\napple’s faceID (deep convolutional networks)\nTesla autonomous car (deep learning)\nvirtual assistant (Siri, Google Assistant)\nNVIDIA DLSS (Deep learning supersampling)\nBloomberg (NLP sentiment analysis)\n\n\n\n\n\n\nFigure 1: General goal of AI1\n\n\n\n\n\n\nData science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.\n\n\n\n\n\n\n\n\nFigure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows.\nIn a perfect world, data scientist will do ML modelling while ML Engineer will productize ML model from Data Scientist.\n\n\n\n\n\n\n\n\nFigure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated! In reality (especially in Small & Medium Enterprise), Data Scientist & ML Engineer jobscrope is intertwine.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "href": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "title": "Overview",
    "section": "",
    "text": "Data science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-scientist-vs-machine-learning-engineer",
    "href": "index.html#data-scientist-vs-machine-learning-engineer",
    "title": "Overview",
    "section": "",
    "text": "Figure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows.\nIn a perfect world, data scientist will do ML modelling while ML Engineer will productize ML model from Data Scientist.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reality-ml-in-production",
    "href": "index.html#reality-ml-in-production",
    "title": "Overview",
    "section": "",
    "text": "Figure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated! In reality (especially in Small & Medium Enterprise), Data Scientist & ML Engineer jobscrope is intertwine.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/license.html",
    "href": "content/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "Text and figures are licensed under Creative Commons Attribution CC BY-SA 4.0. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”."
  },
  {
    "objectID": "content/07_deployment.html",
    "href": "content/07_deployment.html",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "When a data scientist has a model ready, the next step is to deploy it in a way that it can serve the application.\nThe basic meaning of model serving is to host machine-learning models (on the cloud or on premises) and to make their functions available via API so that applications can incorporate AI into their systems.1\nDeploying a machine-learning model in production also involves resource management and model monitoring including operations stats as well as model drifts.\nThings to consider:\n\nAccess points (endpoints): An endpoint is a URL that allows applications to communicate with the target service via HTTPS protocol\nTraffic management: Requests at an endpoint go through various routes, depending on the destination service. Traffic management may also deploy a load-balancing feature to process requests concurrently.\nPre- and post-processing requests: A service may need to transform request messages into the format suitable for the target model and convert response messages into the format required by client applications. Often, serverless functions can handle such transformations.\nMonitor model drifts: We must monitor how each machine-learning model performs and detect when the performance deteriorates and requires retraining.\n\n\n\nServing the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!\n\n\n\nModel monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.\n\n\n\n\n\nA conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#model-serving-strategy",
    "href": "content/07_deployment.html#model-serving-strategy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Serving the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#model-monitor",
    "href": "content/07_deployment.html#model-monitor",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Model monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/07_deployment.html#privacy",
    "href": "content/07_deployment.html#privacy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "A conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "07 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/05_evaluation.html",
    "href": "content/05_evaluation.html",
    "title": "About Learning III",
    "section": "",
    "text": "At the end of “training” (after model optimization), we need to ask the following:\n\nHow well is our model doing?\nIs our model good enough for us to use?\n\nTo answer this, we must do model evaluation. And to evaluate, certain measurement or metric is used to judge (evaluate) the performance of your model. It provides a more interpretable measure of your model’s performance.\nAlso recall that our “learning” output answer will always be in terms of probability.\nThe end goal is to:\n\nOptimal: our trained model(s) performs as well in unseen dataset\nReliable: our trained model(s) behaves as expected\n\n\n\nHere are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class, and the actual value was also positive\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class, and the actual value was also negative\nFalse Positives (FP) is an outcome where the model predicted the positive class incorrectly, and the actual value was negative\nFalse Negatives (FN) is an outcome where the model predicted the negative class incorrectly, and the actual value was positive .\n\n\n\n\n\n\n\nFigure 1: common general metric\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Example 1\n\n\n\n\n\n\n\n\n\nFigure 3: Example 2\n\n\n\n\n\n\n\n\n\nFigure 4: Example 3\n\n\n\nPrecision (or correctness): This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall (sensitivity): a measure of how actual observations are predicted correctly.This shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\nF1-score: is the harmonic mean of the precision and recall values\n\\[\nF1  Score = \\frac{2 \\times Precision \\times  Recall}{Precision +  Recall}\n\\]",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/05_evaluation.html#common-metrics",
    "href": "content/05_evaluation.html#common-metrics",
    "title": "About Learning III",
    "section": "",
    "text": "Here are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class, and the actual value was also positive\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class, and the actual value was also negative\nFalse Positives (FP) is an outcome where the model predicted the positive class incorrectly, and the actual value was negative\nFalse Negatives (FN) is an outcome where the model predicted the negative class incorrectly, and the actual value was positive .\n\n\n\n\n\n\n\nFigure 1: common general metric",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/05_evaluation.html#example",
    "href": "content/05_evaluation.html#example",
    "title": "About Learning III",
    "section": "",
    "text": "Figure 2: Example 1\n\n\n\n\n\n\n\n\n\nFigure 3: Example 2\n\n\n\n\n\n\n\n\n\nFigure 4: Example 3\n\n\n\nPrecision (or correctness): This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall (sensitivity): a measure of how actual observations are predicted correctly.This shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives\n\nF1-score: is the harmonic mean of the precision and recall values\n\\[\nF1  Score = \\frac{2 \\times Precision \\times  Recall}{Precision +  Recall}\n\\]",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html",
    "href": "content/03_data_labelling.html",
    "title": "Overview",
    "section": "",
    "text": "In machine learning, data labeling, or data annotation is the process of identifying raw data (images, text files, videos, etc.) and adding one or more meaningful and informative labels to provide context so that a machine learning model can learn from it.\n\n\nFor classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html#computer-vision-and-audio",
    "href": "content/03_data_labelling.html#computer-vision-and-audio",
    "title": "Overview",
    "section": "",
    "text": "For classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/01_data_collection.html",
    "href": "content/01_data_collection.html",
    "title": "Introduction",
    "section": "",
    "text": "First we must differentiate between data at hand - which is the data that is available to us as data scientist, and data not in hand, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to data collecting.\n\n\nBefore collecting data for your machine learning problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)\n\n\n\n\nMost learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem\n\n\n\n\n\n\nConsider that you have big dataset at hand. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?\nAnswer: Yes! We have statistical approach which we called “Sampling”.\n\n\n\n\n\n\nFigure 4: Sampling from big data\n\n\n\nType of sampling:\n\nrandom sampling : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.\ncluster sampling: we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected\nsystematic sampling: chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’\nstratified random sampling: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.\n\n\n\n\nTo “learn” and achieve good generalization, how much data do we need?\n\nincreasing the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 5: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.2\n\n\n\n\nrelabel this \n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\nEntropy quantifies how much information there is in a random variable, or more specifically its probability distribution.\n\n\n\n\nNow, we already establish criteria of good data & how much data to collect.\nAssume now you already collect data and still facing “imbalanced” where a small fraction of categories have a massive number of samples, and the rest of the categories are associated with only a few samples. What shall we do ?\n\nsimplify image\n\n\n\n\n\n\n\nFigure 6: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right3",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#goodness-of-dataset",
    "href": "content/01_data_collection.html#goodness-of-dataset",
    "title": "Introduction",
    "section": "",
    "text": "Before collecting data for your machine learning problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#general-problem",
    "href": "content/01_data_collection.html#general-problem",
    "title": "Introduction",
    "section": "",
    "text": "Most learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#data-and-sampling",
    "href": "content/01_data_collection.html#data-and-sampling",
    "title": "Introduction",
    "section": "",
    "text": "Consider that you have big dataset at hand. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?\nAnswer: Yes! We have statistical approach which we called “Sampling”.\n\n\n\n\n\n\nFigure 4: Sampling from big data\n\n\n\nType of sampling:\n\nrandom sampling : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.\ncluster sampling: we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected\nsystematic sampling: chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’\nstratified random sampling: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#how-much-data",
    "href": "content/01_data_collection.html#how-much-data",
    "title": "Introduction",
    "section": "",
    "text": "To “learn” and achieve good generalization, how much data do we need?\n\nincreasing the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 5: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.2\n\n\n\n\nrelabel this \n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\nEntropy quantifies how much information there is in a random variable, or more specifically its probability distribution.",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#dealing-with-data",
    "href": "content/01_data_collection.html#dealing-with-data",
    "title": "Introduction",
    "section": "",
    "text": "Now, we already establish criteria of good data & how much data to collect.\nAssume now you already collect data and still facing “imbalanced” where a small fraction of categories have a massive number of samples, and the rest of the categories are associated with only a few samples. What shall we do ?\n\nsimplify image\n\n\n\n\n\n\n\nFigure 6: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right3",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/00_intro-ml-life-cyle.html",
    "href": "content/00_intro-ml-life-cyle.html",
    "title": "Machine Learning Life-Cycle",
    "section": "",
    "text": "Machine Learning Life-Cycle\nSince Machine Learning (ML) is a collection of learning techniques used to extract value from data, we need to know what to say about data-at-hand, how machine learnt and how to evaluate learning model.\n\n\n\n\n\n\nFigure 1: ML Lifecycle1\n\n\n\n\n\n\n\n\nReferences\n\n1. Watson, A. Synthetic data and the data-centric machine learning life cycle. (2022).",
    "crumbs": [
      "Machine Learning Life Cycle"
    ]
  },
  {
    "objectID": "content/02_eda.html",
    "href": "content/02_eda.html",
    "title": "Overview",
    "section": "",
    "text": "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform basic data exploration to study the basic characteristics of a dataset.\nBy studying it basic properties, we may find useful patterns (trend), connections, and relationships within data. This is usually called data exploration or exploratory data analysis (EDA).\nThe whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is good or not.\nOnce we have good data, we can training our machine learning model and get accurate results.\n\n\n\ndescriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot\n\n\n\n\n\n\n\n\nMany times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-analysis",
    "href": "content/02_eda.html#common-data-analysis",
    "title": "Overview",
    "section": "",
    "text": "descriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-pre-processing",
    "href": "content/02_eda.html#common-data-pre-processing",
    "title": "Overview",
    "section": "",
    "text": "Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/04_learning.html",
    "href": "content/04_learning.html",
    "title": "About Learning I",
    "section": "",
    "text": "“The answers you get depend upon the questions you ask.” - Thomas Kuhn.\n\n\n\nLearning in Machine Learning is learning via computational methods using experience (data) to improve performance or to make accurate predictions.1\nThis learning is to automatically (minimal human intervention) discover regularities/patterns in data (through the use of computer algorithms) and with the use of these regularities to take actions (e.g. classifying the data into different categories)\n\n\n\nStatistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.2\nAnother way of understanding statistical learning is, it’s about studying the concept of inference (process of drawing conclusions from data) in both supervised and unsupervised machine learning. Inference covers the entire spectrum of machine learning, from gaining knowledge, making predictions or decisions and constructing models from a set of labeled or unlabeled data.\n\n\n\n\n\n\nFigure 1: Different learning strategy\n\n\n\n\nSupervised Learning: learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into certain task (e.g. classifying data or predicting outcomes accurately)\nUnsupervised Learning: learning strategy that’s looking for hidden or underlying patterns in datasets.\nReinforcement Learning: learning strategy which does not require labeled data or a training set. It relies on the ability to monitor the response to the actions of the learning agent. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n\nAt the end we will measure success of this learning by some statistical model performance. This will tell whether our “trained” model is generalize(model’s ability to adapt properly to new, previously unseen data) well or not,\n\n\n\n\n\n\nFigure 2: Current AI Landscape\n\n\n\nFigure 2 shows current AI opportunities. Andrew Ng highlight the opportunities in:\n\nAI for productivity : usage of Large Language Model (LLM) for office work\nAI for new products and services : to utilised AI to build previously unimaginable services and products.\n\n\nWe shall focus on Supervised Learning\n\n\n\n\nThe basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).\n\n\n\nWe will use the problem of car image classification as a running example to illustrate some basic definitions and to describe the use and evaluation of machine learning algorithms in practice. car image classification is the problem of learning to automatically classify 2 type of car image messages as either Perodua MyVi or Proton Iriz.\n\n\n\nLabel\nDescription\n\n\n\n\n0\nMyVi\n\n\n1\nIriz\n\n\n\n\nExamples: Items or instances of data used for learning or evaluation. In our problem, these examples correspond to the collection of car images we will use for learning and testing.\nFeatures: The set of attributes, often represented as a vector, associated to an example. In the case of image classification, pixels are the raw building blocks of an image. Every image consists of a set of pixels. There is no finer granularity than the pixel. A pixel is considered the “color” or the “intensity” of light that appears in a given place in our image. Various location of this color will give texture, and shape.\n\n\n\n\n\n\n\nFigure 3: Image data representation in vector\n\n\n\n\nLabels: Values or categories assigned to examples. In classification problems, examples are assigned specific categories, for instance, the Perodua MyVi and Proton Iriz categories in our binary classification problem.\nTraining sample: Examples used to train a learning algorithm. In our problem, the training sample consists of a set of car image examples along with their associated labels.\nValidation sample: Examples used to tune the parameters of a learning algorithm when working with labeled data. Learning algorithms typically have one or more free parameters, and the validation sample is used to select appropriate values for these model parameters.\nTest sample: Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available (unseen data) in the learning stage.\nLoss function: A function that measures the difference, or loss, between a predicted label and a true label. Also termed as objective function.\n\\(h\\), (hypothesis): an instance or specific candidate model that maps inputs to outputs and can be evaluated and used to make predictions. A hypothesis is like a smart(well-informed) guess.\n\\(H\\), Hypothesis set: set of conditions of specific candidate model that determines the classification into a group (a function approximation for the target function). It is a statement about the input data and its relation to the class. These “function” is used to associate/estimate or predict the target value, based on the input dataset, algorithm parameters (and hyper-parameters). Mathematically, it is a set of functions mapping features (feature vectors) to the set of labels Y. In our example, these may be a set of functions mapping car image features to Y = {MyVi, Iriz}.\n\n\nLearning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set.3\n\n\n\n\n\n\n\nFigure 4: generic learning process\n\n\n\nIn Figure 4, we show generic statistical learning process.To ensure that our statistical model is going to perform well on new data, we need to consider the following:\n\nUsing “user” prior knowledge, map relevant features of the collection of image into label/category\nPartition the data into a training sample, a validation sample, and a test sample.\nDuring training, example are drawn independently at random\nOur Machine learning algorithm now analyze the examples and produce a classifier,\\(f\\)\ngiven new example from validation, classifier,\\(f\\) predict \\(\\hat{y}=f(x)\\)\nMeasure loss between real truth, \\(y\\) and predicted,\\(\\hat{y}\\)\nIf loss high, repeat (3)-(6) until loss is minimize.\n\n\nWhat happen at step (4) - step (6)\n\nTo have rough idea what’s going on at step (4), consider Figure 5 where our data is label as vector(step (1)).\n\n\n\n\n\n\nFigure 5: data label\n\n\n\n\n\n\n\n\n\nFigure 6: What happened during training\n\n\n\nThus in Figure 6, during 1 individual training loop. Prediction vector is later will be compare with data label using loss function and error is compute.\n\n\n\n\nBias : amount that a model’s prediction differs from the target value, compared to the training data.\nLow Bias: fewer strong assumptions or constraints placed during model training. In this case, the model will closely match the training dataset. Model has ability to capture the true underlying patterns or relationships present in the data\nHigh Bias : model simplifies the underlying patterns too much (strong assumptions or constraints placed during model training). In this case, the model will not match the training dataset closely. Usually called under-fitting\nVariance: the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\nLow Variance : not overly influenced by the noise or fluctuations in the training data. It tends to be more stable, providing consistent predictions even when trained on different subsets of the data.\nHigh Variance : overly sensitive to the noise or fluctuations in the training data. It tries to fit the training data so closely that it captures not only the true underlying patterns but also the random noise in the data. Usually called over-fitting\nHigh Bias and Low Variance: Consistent but inaccurate due to oversimplification. \\(\\Longrightarrow\\) poor performance\nHigh Variance and Low Bias: Accurate but inconsistent, capturing noise and over-fitting to the training data. \\(\\Longrightarrow\\) poor performance\n\n\n\n\n\n\n\nFigure 7: What happened during training\n\n\n\nOverfitting is when your model is basically memorising the data without really understanding/interpolating a general function that would apply to any external data. In other words it creates a function that is tailored specifically to the training dataset thus it would perform near perfectly with the training data, but it would perform quite badly with any data that it hasn’t seen before. (models the training data too well but fails to generalize to new data)\nUnderfitting is simply when you do not have enough data for the model to create/interpolate a general function that describes the process. This leads to the model performing badly for most input data. (model that fails to capture the underlying pattern in the training data)\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 8: Training trade-off4\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#big-picture",
    "href": "content/04_learning.html#big-picture",
    "title": "About Learning I",
    "section": "",
    "text": "Learning in Machine Learning is learning via computational methods using experience (data) to improve performance or to make accurate predictions.1\nThis learning is to automatically (minimal human intervention) discover regularities/patterns in data (through the use of computer algorithms) and with the use of these regularities to take actions (e.g. classifying the data into different categories)",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#statistical-learning",
    "href": "content/04_learning.html#statistical-learning",
    "title": "About Learning I",
    "section": "",
    "text": "Statistical learning refers to a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data.2\nAnother way of understanding statistical learning is, it’s about studying the concept of inference (process of drawing conclusions from data) in both supervised and unsupervised machine learning. Inference covers the entire spectrum of machine learning, from gaining knowledge, making predictions or decisions and constructing models from a set of labeled or unlabeled data.\n\n\n\n\n\n\nFigure 1: Different learning strategy\n\n\n\n\nSupervised Learning: learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into certain task (e.g. classifying data or predicting outcomes accurately)\nUnsupervised Learning: learning strategy that’s looking for hidden or underlying patterns in datasets.\nReinforcement Learning: learning strategy which does not require labeled data or a training set. It relies on the ability to monitor the response to the actions of the learning agent. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n\nAt the end we will measure success of this learning by some statistical model performance. This will tell whether our “trained” model is generalize(model’s ability to adapt properly to new, previously unseen data) well or not,\n\n\n\n\n\n\nFigure 2: Current AI Landscape\n\n\n\nFigure 2 shows current AI opportunities. Andrew Ng highlight the opportunities in:\n\nAI for productivity : usage of Large Language Model (LLM) for office work\nAI for new products and services : to utilised AI to build previously unimaginable services and products.\n\n\nWe shall focus on Supervised Learning",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "href": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "title": "About Learning I",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#example",
    "href": "content/04_learning.html#example",
    "title": "About Learning I",
    "section": "",
    "text": "We will use the problem of car image classification as a running example to illustrate some basic definitions and to describe the use and evaluation of machine learning algorithms in practice. car image classification is the problem of learning to automatically classify 2 type of car image messages as either Perodua MyVi or Proton Iriz.\n\n\n\nLabel\nDescription\n\n\n\n\n0\nMyVi\n\n\n1\nIriz\n\n\n\n\nExamples: Items or instances of data used for learning or evaluation. In our problem, these examples correspond to the collection of car images we will use for learning and testing.\nFeatures: The set of attributes, often represented as a vector, associated to an example. In the case of image classification, pixels are the raw building blocks of an image. Every image consists of a set of pixels. There is no finer granularity than the pixel. A pixel is considered the “color” or the “intensity” of light that appears in a given place in our image. Various location of this color will give texture, and shape.\n\n\n\n\n\n\n\nFigure 3: Image data representation in vector\n\n\n\n\nLabels: Values or categories assigned to examples. In classification problems, examples are assigned specific categories, for instance, the Perodua MyVi and Proton Iriz categories in our binary classification problem.\nTraining sample: Examples used to train a learning algorithm. In our problem, the training sample consists of a set of car image examples along with their associated labels.\nValidation sample: Examples used to tune the parameters of a learning algorithm when working with labeled data. Learning algorithms typically have one or more free parameters, and the validation sample is used to select appropriate values for these model parameters.\nTest sample: Examples used to evaluate the performance of a learning algorithm. The test sample is separate from the training and validation data and is not made available (unseen data) in the learning stage.\nLoss function: A function that measures the difference, or loss, between a predicted label and a true label. Also termed as objective function.\n\\(h\\), (hypothesis): an instance or specific candidate model that maps inputs to outputs and can be evaluated and used to make predictions. A hypothesis is like a smart(well-informed) guess.\n\\(H\\), Hypothesis set: set of conditions of specific candidate model that determines the classification into a group (a function approximation for the target function). It is a statement about the input data and its relation to the class. These “function” is used to associate/estimate or predict the target value, based on the input dataset, algorithm parameters (and hyper-parameters). Mathematically, it is a set of functions mapping features (feature vectors) to the set of labels Y. In our example, these may be a set of functions mapping car image features to Y = {MyVi, Iriz}.\n\n\nLearning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set.3\n\n\n\n\n\n\n\nFigure 4: generic learning process\n\n\n\nIn Figure 4, we show generic statistical learning process.To ensure that our statistical model is going to perform well on new data, we need to consider the following:\n\nUsing “user” prior knowledge, map relevant features of the collection of image into label/category\nPartition the data into a training sample, a validation sample, and a test sample.\nDuring training, example are drawn independently at random\nOur Machine learning algorithm now analyze the examples and produce a classifier,\\(f\\)\ngiven new example from validation, classifier,\\(f\\) predict \\(\\hat{y}=f(x)\\)\nMeasure loss between real truth, \\(y\\) and predicted,\\(\\hat{y}\\)\nIf loss high, repeat (3)-(6) until loss is minimize.\n\n\nWhat happen at step (4) - step (6)\n\nTo have rough idea what’s going on at step (4), consider Figure 5 where our data is label as vector(step (1)).\n\n\n\n\n\n\nFigure 5: data label\n\n\n\n\n\n\n\n\n\nFigure 6: What happened during training\n\n\n\nThus in Figure 6, during 1 individual training loop. Prediction vector is later will be compare with data label using loss function and error is compute.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#interpret-step-6",
    "href": "content/04_learning.html#interpret-step-6",
    "title": "About Learning I",
    "section": "",
    "text": "Bias : amount that a model’s prediction differs from the target value, compared to the training data.\nLow Bias: fewer strong assumptions or constraints placed during model training. In this case, the model will closely match the training dataset. Model has ability to capture the true underlying patterns or relationships present in the data\nHigh Bias : model simplifies the underlying patterns too much (strong assumptions or constraints placed during model training). In this case, the model will not match the training dataset closely. Usually called under-fitting\nVariance: the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\nLow Variance : not overly influenced by the noise or fluctuations in the training data. It tends to be more stable, providing consistent predictions even when trained on different subsets of the data.\nHigh Variance : overly sensitive to the noise or fluctuations in the training data. It tries to fit the training data so closely that it captures not only the true underlying patterns but also the random noise in the data. Usually called over-fitting\nHigh Bias and Low Variance: Consistent but inaccurate due to oversimplification. \\(\\Longrightarrow\\) poor performance\nHigh Variance and Low Bias: Accurate but inconsistent, capturing noise and over-fitting to the training data. \\(\\Longrightarrow\\) poor performance\n\n\n\n\n\n\n\nFigure 7: What happened during training\n\n\n\nOverfitting is when your model is basically memorising the data without really understanding/interpolating a general function that would apply to any external data. In other words it creates a function that is tailored specifically to the training dataset thus it would perform near perfectly with the training data, but it would perform quite badly with any data that it hasn’t seen before. (models the training data too well but fails to generalize to new data)\nUnderfitting is simply when you do not have enough data for the model to create/interpolate a general function that describes the process. This leads to the model performing badly for most input data. (model that fails to capture the underlying pattern in the training data)\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 8: Training trade-off4\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html",
    "href": "content/06_machine_learning.html",
    "title": "What is Machine Learning",
    "section": "",
    "text": "Supervised Learning\nIs to map input onto sets of output, which had been pre-determined by the “supervisor” or researcher/analyzer. Hence called “supervised”. The problem is \\(X \\mapsto Y\\). In another word, we have \\(X\\) as input space, and \\(Y\\) output space, and the learning is a process of mapping from \\(X\\) to \\(Y\\).\nExamples:\n\nLearning associations Association rule, where given X, we want to know Y, or \\(Pr(Y|X)\\)\nClassification Classification rule, where: IF \\(X_1 &gt; a\\) AND \\(X_2 &gt; b\\) THEN Y = 0 else Y = 1\nPattern recognition This is by way of patterns, such as “pixels” in image, “frequencies” in sound and speech, “medical records and data” in medical diagnosis, biometrics in identification, etc. IF pattern \\(X_1\\) matches pattern \\(X_2\\), then \\(Y = k\\)\nKnowledge extraction Using models, such as Linear Model to predict information given the data. \\(Y = \\beta_0 + \\beta_1X_1 +....\\)\n\nUnsupervised learning\nWe have only \\(Xs\\), input data, and no output to map to. \\(Y\\) may exist, but it is “latent” (i.e. not directly observable)\n\nClustering Simple clustering procedure, \\(Pr(X_i = k)\\) given a certain rule, to discover similarities across groupings. E.g. Customer segmentation, density estimation, image compression, etc.\nDiscriminant To “discriminate” \\(Xs\\) across distinct groupings by using certain measures E.g. Principle Components, Linear Discriminant, etc.\n\nReinforcement Learning\nThe output \\(Y\\) is a set of actions, which is ordered in a sequence \\((a_ia_ja_k...a_n)\\). The objective is to “learn” which set of actions sequence which maximizes the rewards, \\(Z\\). \\(X\\) now is called the possible states or set of states. \\(Y\\) is the game theoretic action space choosing the states, and \\(Z\\) is the reward space, given various state space sequences or paths. Here the “learning” is by process of learning from various paths, to discover the paths which gives optimal rewards. An example of this a chess game between two players. Here, we could say generally that the domain is both “game theory” involving “multiple agents” which involve choices which are probabilistic in nature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve saw our scores. Let’s sort them.\nRandom Forest Classification %87 Accuracy\nNaive Bayes and KNN Classification %81 Accuracy\nSupport Vector Machine Classification %80 Accuracy\nDecision Tree Classification %78 Accuracy\nLogistic Regression %74 Accuracy\nAs we can see, for classification, the best algorithm is Random Forest Classification.\n\nBut do not forget that, in different datasets, it will be a different algorithm So it does not show that, Random Forest Classification is not the best algorithm for classification - however for this dataset it is :)\n\nMake your own observations.\n\n\n\n\nAccuracy\n\n\n\n\nNaive Bayes\n0.776042\n\n\nSupport Vector Machine\n0.770796\n\n\nLogistic Regression\n0.766917\n\n\nK Nearest Neighbor\n0.706955\n\n\nDecision Tree\n0.706955\n\n\n\nFrom the above output, it is clear that for this dataset, SVM, Logistic Regression and Naive Bayes works better….\n\n\n\nYou can re-run the Rmd file, and you can see changes happen in the plots. But doesn’t matter how many time you run it, the main conclusions doesn’t change!!! If you do this, effectively, you are repeating the simulations over and over again, which cause some minor changes to the results (i.e. due to randomize errors or white noise), but the conclusions remains the same.\n\n\n\n\n\n\n\n\n\nFigure 1: Sampling from big data\n\n\n\nInterpretability — If a business wants high model transparency and wants to understand exactly why and how the model is generating predictions, they need to observe the inner mechanics of the AI/ML method. This leads to interpreting the model’s weights and features to determine the given output. This is interpretability.\nHigh interpretability typically comes at the cost of performance, as seen in the following figure. If we wants to achieve high performance but still wants to have a general understanding of the model behavior, model explainability starts to play a larger role.\nExplainability — Explainability is how to take an ML model and explain the behavior in human terms.\nWhen datasets are large and the data is related to images or text, neural networks can meet the customer’s AI/ML objective with high performance. In such cases, where complex methods are required to maximize performance, data scientists may focus on model explainability instead of interpretability.\n\n\n\n\n\n\nFigure 2: The trade-off between accuracy and interpretability.\n\n\n\nFigure 2 shows the trade-off between accuracy and interpretability of machine learning algorithms.\nModel interpretability is mainly related to the complexity of ML that is used to build the model. Generally speaking, models with more complex ML algorithms are more challenging to interpret and explain. In contrast, models with more straightforward ML algorithms are easiest to interpret and explain. Therefore, ML models can be categorized into two criteria: intrinsic or post hoc.\nIntrinsic interpretability model uses transparent ML model, such as sparse linear models or short decision trees. Interpretability is achieved by limiting the ML model complexity in such a way that the output is understandable by humans. Unfortunately, in most cases, the best performance of ML models belongs to very complex ML algorithms. In other words, model accuracy requires more complex ML algorithms, and simple ML algorithms might not make the most accurate prediction. Therefore, intrinsic models are suitable when interpretability is more important than accuracy.\nIn contrast, post hoc interpretability model is used to explain complex/black-box ML models. After the ML models have been trained, the interpretation method is applied to extract information from the trained ML model without precisely depending on how they work.\n\n\n\nA popular definition of interpretability frequently used by researchers is “interpretability in machine learning is a degree to which a human can understand the cause of a decision from an ML model”. It can also be defined as “the ability to explain the model outcome in understandable ways to a human. So far, no mathematical definition. The primary goal of interpretability is to explain the model outcomes in a way that is easy for users to understand.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#comparing-different-classification-algorithms",
    "href": "content/06_machine_learning.html#comparing-different-classification-algorithms",
    "title": "What is Machine Learning",
    "section": "",
    "text": "We’ve saw our scores. Let’s sort them.\nRandom Forest Classification %87 Accuracy\nNaive Bayes and KNN Classification %81 Accuracy\nSupport Vector Machine Classification %80 Accuracy\nDecision Tree Classification %78 Accuracy\nLogistic Regression %74 Accuracy\nAs we can see, for classification, the best algorithm is Random Forest Classification.\n\nBut do not forget that, in different datasets, it will be a different algorithm So it does not show that, Random Forest Classification is not the best algorithm for classification - however for this dataset it is :)\n\nMake your own observations.\n\n\n\n\nAccuracy\n\n\n\n\nNaive Bayes\n0.776042\n\n\nSupport Vector Machine\n0.770796\n\n\nLogistic Regression\n0.766917\n\n\nK Nearest Neighbor\n0.706955\n\n\nDecision Tree\n0.706955\n\n\n\nFrom the above output, it is clear that for this dataset, SVM, Logistic Regression and Naive Bayes works better….",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#simulating-the-exercise",
    "href": "content/06_machine_learning.html#simulating-the-exercise",
    "title": "What is Machine Learning",
    "section": "",
    "text": "You can re-run the Rmd file, and you can see changes happen in the plots. But doesn’t matter how many time you run it, the main conclusions doesn’t change!!! If you do this, effectively, you are repeating the simulations over and over again, which cause some minor changes to the results (i.e. due to randomize errors or white noise), but the conclusions remains the same.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#interpretability-vs-flexibility",
    "href": "content/06_machine_learning.html#interpretability-vs-flexibility",
    "title": "What is Machine Learning",
    "section": "",
    "text": "Figure 1: Sampling from big data\n\n\n\nInterpretability — If a business wants high model transparency and wants to understand exactly why and how the model is generating predictions, they need to observe the inner mechanics of the AI/ML method. This leads to interpreting the model’s weights and features to determine the given output. This is interpretability.\nHigh interpretability typically comes at the cost of performance, as seen in the following figure. If we wants to achieve high performance but still wants to have a general understanding of the model behavior, model explainability starts to play a larger role.\nExplainability — Explainability is how to take an ML model and explain the behavior in human terms.\nWhen datasets are large and the data is related to images or text, neural networks can meet the customer’s AI/ML objective with high performance. In such cases, where complex methods are required to maximize performance, data scientists may focus on model explainability instead of interpretability.\n\n\n\n\n\n\nFigure 2: The trade-off between accuracy and interpretability.\n\n\n\nFigure 2 shows the trade-off between accuracy and interpretability of machine learning algorithms.\nModel interpretability is mainly related to the complexity of ML that is used to build the model. Generally speaking, models with more complex ML algorithms are more challenging to interpret and explain. In contrast, models with more straightforward ML algorithms are easiest to interpret and explain. Therefore, ML models can be categorized into two criteria: intrinsic or post hoc.\nIntrinsic interpretability model uses transparent ML model, such as sparse linear models or short decision trees. Interpretability is achieved by limiting the ML model complexity in such a way that the output is understandable by humans. Unfortunately, in most cases, the best performance of ML models belongs to very complex ML algorithms. In other words, model accuracy requires more complex ML algorithms, and simple ML algorithms might not make the most accurate prediction. Therefore, intrinsic models are suitable when interpretability is more important than accuracy.\nIn contrast, post hoc interpretability model is used to explain complex/black-box ML models. After the ML models have been trained, the interpretation method is applied to extract information from the trained ML model without precisely depending on how they work.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/06_machine_learning.html#interpretability-in-machine-learning",
    "href": "content/06_machine_learning.html#interpretability-in-machine-learning",
    "title": "What is Machine Learning",
    "section": "",
    "text": "A popular definition of interpretability frequently used by researchers is “interpretability in machine learning is a degree to which a human can understand the cause of a decision from an ML model”. It can also be defined as “the ability to explain the model outcome in understandable ways to a human. So far, no mathematical definition. The primary goal of interpretability is to explain the model outcomes in a way that is easy for users to understand.",
    "crumbs": [
      "06 - Machine Learning"
    ]
  },
  {
    "objectID": "content/08_exciting_ai.html",
    "href": "content/08_exciting_ai.html",
    "title": "Exciting AI",
    "section": "",
    "text": "Exciting AI\nWhat is exciting AI today?\n\ntransformer\nLLM\nimage generation",
    "crumbs": [
      "08 - Exciting AI Today"
    ]
  },
  {
    "objectID": "content/use_case.html",
    "href": "content/use_case.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "So far we have learnt"
  },
  {
    "objectID": "content/use_case.html#model-trade-off",
    "href": "content/use_case.html#model-trade-off",
    "title": "Supervised Learning",
    "section": "Model Trade-Off",
    "text": "Model Trade-Off\n\n\n\n\n\n\nFigure 2: Trade-off between model interpretability and performance\n\n\n\nWhen building a model, things to consider:"
  },
  {
    "objectID": "content/use_case.html#use-case",
    "href": "content/use_case.html#use-case",
    "title": "Supervised Learning",
    "section": "Use Case",
    "text": "Use Case\n\nCase-1: Face Recognition/Verification\n\n\nCase-2: Weather(Rain) Prediction\n\n\nCase-3: Housing Price Prediction"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]