[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "AI is everywhere!\nSome notable examples:\n\ngoogle lens (region proposal network)\ngoogle translate (Transformer-based)\nYouTube automatic captioning (automatic speech recognition)\nGmail spam filters (rule-based filters + Density clustering)\napple’s faceID (deep convolutional networks)\nTesla autonomous car (deep learning)\nvirtual assistant (Siri, Google Assistant)\nNVIDIA DLSS (Deep learning supersampling)\nBloomberg (NLP sentiment analysis)\n\n\n\n\n\n\nFigure 1: General goal of AI1\n\n\n\n\n\n\nData science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.\n\n\n\n\n\n\n\n\nFigure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows.\nIn a perfect world, data scientist will do ML modelling while ML Engineer will productize ML model from Data Scientist.\n\n\n\n\n\n\n\n\nFigure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated! In reality (especially in Small & Medium Enterprise), Data Scientist & ML Engineer jobscrope is intertwine.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "href": "index.html#artificial-intelligence-machine-learning-and-deep-learning",
    "title": "Overview",
    "section": "",
    "text": "Data science and statistics - are two of the same, except that in earlier days, Data Science as we know it today, was called “statistical data analysis” or “applied statistics”.\n“Data Scientist” means a professional who uses scientific methods to liberate and create meaning from raw data.\n“Statistics” means the practice or science of collecting and analyzing numerical data in large quantities.\nThere are no real difference between the two, except that “Data Scientists” prowes in large scale data or Big Data and fast computing. Otherwise, they are the same.\nToday, there are no difference between the two.2\n\n\n\n\n\nFigure 2: Everything everywhere all at once3\n\n\n\n\n\n\n\n\n\nFigure 3: Artificial intelligence, machine learning, and data science.4\n\n\n\n\nDeep learning is a subfield of machine learning, which is, in turn, a subfield of artificial intelligence (AI).\nThe central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\nMachine learning is subfield tends to be specifically interested in pattern recognition and learning from data.\nArtificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain.\nDeep learning is an approach to AI. It is a type of machine learning, a technique that allows computer systems to improve with experience and data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-scientist-vs-machine-learning-engineer",
    "href": "index.html#data-scientist-vs-machine-learning-engineer",
    "title": "Overview",
    "section": "",
    "text": "Figure 4: domain area of deep learning5\n\n\n\n\nMLOps is the process of automating and productionalizing machine learning applications and workflows.\nIn a perfect world, data scientist will do ML modelling while ML Engineer will productize ML model from Data Scientist.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reality-ml-in-production",
    "href": "index.html#reality-ml-in-production",
    "title": "Overview",
    "section": "",
    "text": "Figure 6: Only a small fraction of real-world ML systems is composed of the ML code, as shown by the small black box in the middle. The required surrounding infrastructure is vast and complex.6\n\n\n\n\nMachine learning in production is very complicated! In reality (especially in Small & Medium Enterprise), Data Scientist & ML Engineer jobscrope is intertwine.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/license.html",
    "href": "content/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "Text and figures are licensed under Creative Commons Attribution CC BY-SA 4.0. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”."
  },
  {
    "objectID": "content/05_evaluation.html",
    "href": "content/05_evaluation.html",
    "title": "About Learning III",
    "section": "",
    "text": "At the end of “training” (after model optimization), we need to ask the following:\n\nHow well is our model doing?\nIs our model good enough for us to use?\n\nTo answer this, we must do model evaluation. And to evaluate, certain measurement or metric is used to judge (evaluate) the performance of your model. It provides a more interpretable measure of your model’s performance.\nAlso recall that our “learning” output answer will always be in terms of probability.\n\n\nHere are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class.\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class\nFalse Positives (FP) is an outcome where the model incorrectly predicts the positive class.\nFalse Negatives (FN) is an outcome where the model incorrectly predicts the negative class.\n\n\n\n\n\n\n\nFigure 1: common general metric\n\n\n\nPrecision: This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall: metric that shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/05_evaluation.html#common-metrics",
    "href": "content/05_evaluation.html#common-metrics",
    "title": "About Learning III",
    "section": "",
    "text": "Here are the essential metrics:\n\nTrue Positives (TP) is an outcome where the model correctly predicts the positive class.\nTrue Negatives (TN) is an outcome where the model correctly predicts the negative class\nFalse Positives (FP) is an outcome where the model incorrectly predicts the positive class.\nFalse Negatives (FN) is an outcome where the model incorrectly predicts the negative class.\n\n\n\n\n\n\n\nFigure 1: common general metric\n\n\n\nPrecision: This metric shows how often your model is correct when predicting the target class.\n\\[\nPrecision = \\frac{TP}{TP + FP}\n\\] Recall: metric that shows whether your model can find all objects of the target class(how many correct items were found compared to how many were actually there)\n\\[\nRecall = \\frac{TP}{TP + FN}\n\\]\n\nHigh precision and high recall mean that your model is performing well.\nLow precision means that your model will predict some false positives\nLow recall means that your model will predict some false negatives",
    "crumbs": [
      "05 - Model Evaluation"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html",
    "href": "content/03_data_labelling.html",
    "title": "Overview",
    "section": "",
    "text": "In machine learning, data labeling, or data annotation is the process of identifying raw data (images, text files, videos, etc.) and adding one or more meaningful and informative labels to provide context so that a machine learning model can learn from it.\n\n\nFor classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/03_data_labelling.html#computer-vision-and-audio",
    "href": "content/03_data_labelling.html#computer-vision-and-audio",
    "title": "Overview",
    "section": "",
    "text": "For classification problem, organize your dataset according to the following structure:\n├── train\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n├── valid\n│   ├── category 1\n|      ├── 1.jpg\n│      ├── 2.jpg\n│   ├── category 2\n|      ├── 1.jpg\n│      ├── 2.jpg\n\nIn PyTorch, ImageFolder can be use to automatically label your data. In Tensorflow, similar class, image_dataset_from_directory can also be use.\nFor object detection problem, we can use several tool such as label-studio, labelImg, labelme, etc.\n\n\n\n\n\n\nFigure 1: label object in image",
    "crumbs": [
      "03 - Data Labelling"
    ]
  },
  {
    "objectID": "content/01_data_collection.html",
    "href": "content/01_data_collection.html",
    "title": "Introduction",
    "section": "",
    "text": "First we must differentiate between data at hand - which is the data that is available to us as data scientist, and data not in hand, which are data not yet available or will come in the future whereby the model will be applied on. True reliability of the model will be when tested against data not in hand. To understand this, we need to go back to data collecting.\n\n\nBefore collecting data for your machine learning problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)\n\n\n\n\nMost learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem\n\n\n\n\n\n\nConsider that you have big dataset at hand. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?\nAnswer: Yes! We have statistical approach which we called “Sampling”.\n\n\n\n\n\n\nFigure 4: Sampling from big data\n\n\n\nType of sampling:\n\nrandom sampling : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.\ncluster sampling: we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected\nsystematic sampling: chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’\nstratified random sampling: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.\n\n\n\n\nTo “learn” and achieve good generalization, how much data do we need?\n\nincreasing the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 5: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.2\n\n\n\n\nrelabel this \n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\nEntropy quantifies how much information there is in a random variable, or more specifically its probability distribution.\n\n\n\n\nNow, we already establish criteria of good data & how much data to collect.\nAssume now you already collect data and still facing “imbalanced” where a small fraction of categories have a massive number of samples, and the rest of the categories are associated with only a few samples. What shall we do ?\n\nsimplify image\n\n\n\n\n\n\n\nFigure 6: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right3",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#goodness-of-dataset",
    "href": "content/01_data_collection.html#goodness-of-dataset",
    "title": "Introduction",
    "section": "",
    "text": "Before collecting data for your machine learning problem, we first need to know “what makes a good dataset”?\n\nquality (representative and high-quality of inputs data)\nquantity (consistent and accurate labels on target data/ground truth)\nvariability (reflect post deployment changes)",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#general-problem",
    "href": "content/01_data_collection.html#general-problem",
    "title": "Introduction",
    "section": "",
    "text": "Most learning systems usually assume (e.g. academic, kaggle data) that training datasets used for learning are balanced.\nHowever, in real-world applications, training samples( data at hand ) typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few samples1.\n\n\n\n\n\n\nFigure 1: Long Tail Data Illustration\n\n\n\nLong-tail data is visually represented by a hyperbolic curve like in Figure 1.\nSo long-tail data is the collection of all data about items that serve a specific niche and have a low demand but exist in greater varieties.\nConsider example, in autonomous driving, you would want a model detecting pedestrians to work equally well, irrespective of the weather, visual conditions, how fast the pedestrian is moving, how occluded they are, et cetera. Most likely however, your model will perform much worse on cases that are more rare—for example, a baby stroller swiftly emerging from behind a parked car in unpleasant weather.\nThe point of failure here is that the model has been trained on data that was recorded during regular traffic conditions. As a result, the representation of these rare scenarios (as a portion of the entire training dataset) is much lower compared to common scenarios. Figure 2 is an example of two highway scenarios, whereas lane detection will be significantly more difficult in the right hand picture compared to the left.\n\n\n\n\n\n\nFigure 2: long tail scenario\n\n\n\nThus, need to acquire more of these rare cases in our training data!\n\n\n\n\n\n\nFigure 3: class imbalance problem",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#data-and-sampling",
    "href": "content/01_data_collection.html#data-and-sampling",
    "title": "Introduction",
    "section": "",
    "text": "Consider that you have big dataset at hand. Is there a way to pick a subset of the dataset and that can be a good representation of the entire dataset?\nAnswer: Yes! We have statistical approach which we called “Sampling”.\n\n\n\n\n\n\nFigure 4: Sampling from big data\n\n\n\nType of sampling:\n\nrandom sampling : every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.\ncluster sampling: we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups and a whole subgroups is randomly selected\nsystematic sampling: chooses member from a target population by selecting a random starting point and selects sample members after a fixed ‘sampling interval.’\nstratified random sampling: divide the population into subgroups (called strata) based on different traits like category, then we select the sample(s) from these subgroup.",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#how-much-data",
    "href": "content/01_data_collection.html#how-much-data",
    "title": "Introduction",
    "section": "",
    "text": "To “learn” and achieve good generalization, how much data do we need?\n\nincreasing the dataset sample size is a reduction in model over-fitting (avoid the model “memorize”)\nbe careful of noise, outliers, and irrelevant information in additional data (recall “Goodness of dataset”)\n\n\n\n\n\n\n\nFigure 5: Model performance of deep learning vs. other machine learning algorithms as a function of number of samples.2\n\n\n\n\nrelabel this \n\n\n“To answer the “how much data is enough” question, it’s absolutely true that no machine learning expert can predict how much data is needed. The only way to find out out is to set a hypothesis and to test it on a real case.” — Maksym Tatariants\n\nWe always prefer large amount of data, but how large is large, and how big is big? This is a problem of sufficiency, because even though the data may be large, but contains insufficient entropy, will render the data to be small, despite the large size in bytes.\n\nEntropy quantifies how much information there is in a random variable, or more specifically its probability distribution.",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "content/01_data_collection.html#dealing-with-data",
    "href": "content/01_data_collection.html#dealing-with-data",
    "title": "Introduction",
    "section": "",
    "text": "Now, we already establish criteria of good data & how much data to collect.\nAssume now you already collect data and still facing “imbalanced” where a small fraction of categories have a massive number of samples, and the rest of the categories are associated with only a few samples. What shall we do ?\n\nsimplify image\n\n\n\n\n\n\n\nFigure 6: Three main categories of approaches proposed and tested for tackling the class imbalance problem. Main categories of approaches on the left, followed by subcategories and some examples on the right3",
    "crumbs": [
      "01 - Data Collection"
    ]
  },
  {
    "objectID": "02_eda - Copy.html",
    "href": "02_eda - Copy.html",
    "title": "Overview",
    "section": "",
    "text": "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform basic data exploration to study the basic characteristics of a dataset.\nBy studying it basic properties, we may find useful patterns, connections, and relationships within data. This is usually called data exploration or exploratory data analysis (EDA).\nThe whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is good or not.\nOnce we have good data, we can training our machine learning model and get accurate results.\n\n\nCommon type of dataset is:\n\ncategorical data (image, voice, videos)\ntabular/numerical data\ntime series\ntext\n\n\n\n\n\n\nMany times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nData augmentation is used when we want to add “organized entropy” into an existing data set. This is only meaningful if those entropy helps to expand the data at hand, without altering the “meaningful aspects” of the data. That’s why it is useful in image processing or voice, which structures are not altered, but increase the space and dimensions.\n\n\n\nOpposite to augmentation is “de-noising”, where we apply filters to take out the noises in the data. The argument here is reverse that is to reduce entropy in the data. Again, this is meaningful if there are no alterations to the basic structure of the data; and hence useful in image or voice processing. In NLP, removal of stop-words is a de-noising exercise.\n\n\n\nData pre-processing may involve all of the above: transformation, augmentation and de-noising. In some cases all are required and helpful, in some cases a mixture of them will do. The basic process however is always data transformation.\n\n\n\nDimensionality Reduction is a method of mapping a set of data onto a smaller space, represented by unique mapping between the raw data and a vector space, which serves as a “look-up table”. This reduction does not alter the structure of the data, instead it just compressed the data into a smaller space in terms of computer memories. Instead of working with raw data, we deal with its “essence” representations. An example of this is tokenization in NLP.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.\n\n\n\n\n\n\nImagine you have mnist image data below.\nNow, since our data now is images and not tabular. We can use simple histogram plot like ?@fig-mnist-count to view each class of MNIST dataset.\n\nWe can observe that data distribution almost the same (almost balanced) for each class\n\n\n\n\n\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\nFigure 1: correlation plot\n\n\n\nSo, in statistical terms we use correlation to denote association between two quantitative variables.\n\n\nImagine you have tabular data as below.\nDataset in ?@tbl-dummy-data have 7 columns \\(\\Longrightarrow\\) 7 features\nQuestion : From Figure 1, what is the best way to describe or visualize the data given to us? Answer: Let’s do pair-plot (combination of scatter plot)\nWhat can we say about ?@fig-pair-plot ?\n\n\n\nBut if we have many features like ?@tbl-big-dummy-data and want to plot pair-plot like ?@fig-big-pair-plot, seem to overwhelming and confuse isn’t?\nSolution? Use correlation heatmap \\(\\Longrightarrow\\) easier to see based on correlation value/coefficient, r (recall our Figure 1).r value is the degree of association.\nNow, based on ?@tbl-correlation, let change our ?@fig-big-pair-plot to correlation heatmap\n\n\n\n\n\n\nImagine you have time-series data as below."
  },
  {
    "objectID": "02_eda - Copy.html#dataset-source-type",
    "href": "02_eda - Copy.html#dataset-source-type",
    "title": "Overview",
    "section": "",
    "text": "Common type of dataset is:\n\ncategorical data (image, voice, videos)\ntabular/numerical data\ntime series\ntext"
  },
  {
    "objectID": "02_eda - Copy.html#common-data-pre-processing",
    "href": "02_eda - Copy.html#common-data-pre-processing",
    "title": "Overview",
    "section": "",
    "text": "Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nData augmentation is used when we want to add “organized entropy” into an existing data set. This is only meaningful if those entropy helps to expand the data at hand, without altering the “meaningful aspects” of the data. That’s why it is useful in image processing or voice, which structures are not altered, but increase the space and dimensions.\n\n\n\nOpposite to augmentation is “de-noising”, where we apply filters to take out the noises in the data. The argument here is reverse that is to reduce entropy in the data. Again, this is meaningful if there are no alterations to the basic structure of the data; and hence useful in image or voice processing. In NLP, removal of stop-words is a de-noising exercise.\n\n\n\nData pre-processing may involve all of the above: transformation, augmentation and de-noising. In some cases all are required and helpful, in some cases a mixture of them will do. The basic process however is always data transformation.\n\n\n\nDimensionality Reduction is a method of mapping a set of data onto a smaller space, represented by unique mapping between the raw data and a vector space, which serves as a “look-up table”. This reduction does not alter the structure of the data, instead it just compressed the data into a smaller space in terms of computer memories. Instead of working with raw data, we deal with its “essence” representations. An example of this is tokenization in NLP.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove."
  },
  {
    "objectID": "02_eda - Copy.html#categorical-data",
    "href": "02_eda - Copy.html#categorical-data",
    "title": "Overview",
    "section": "",
    "text": "Imagine you have mnist image data below.\nNow, since our data now is images and not tabular. We can use simple histogram plot like ?@fig-mnist-count to view each class of MNIST dataset.\n\nWe can observe that data distribution almost the same (almost balanced) for each class"
  },
  {
    "objectID": "02_eda - Copy.html#tabular-data",
    "href": "02_eda - Copy.html#tabular-data",
    "title": "Overview",
    "section": "",
    "text": "scatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\nFigure 1: correlation plot\n\n\n\nSo, in statistical terms we use correlation to denote association between two quantitative variables.\n\n\nImagine you have tabular data as below.\nDataset in ?@tbl-dummy-data have 7 columns \\(\\Longrightarrow\\) 7 features\nQuestion : From Figure 1, what is the best way to describe or visualize the data given to us? Answer: Let’s do pair-plot (combination of scatter plot)\nWhat can we say about ?@fig-pair-plot ?\n\n\n\nBut if we have many features like ?@tbl-big-dummy-data and want to plot pair-plot like ?@fig-big-pair-plot, seem to overwhelming and confuse isn’t?\nSolution? Use correlation heatmap \\(\\Longrightarrow\\) easier to see based on correlation value/coefficient, r (recall our Figure 1).r value is the degree of association.\nNow, based on ?@tbl-correlation, let change our ?@fig-big-pair-plot to correlation heatmap"
  },
  {
    "objectID": "02_eda - Copy.html#time-series-data",
    "href": "02_eda - Copy.html#time-series-data",
    "title": "Overview",
    "section": "",
    "text": "Imagine you have time-series data as below."
  },
  {
    "objectID": "content/00_intro-ml-life-cyle.html",
    "href": "content/00_intro-ml-life-cyle.html",
    "title": "Machine Learning Life-Cycle",
    "section": "",
    "text": "Machine Learning Life-Cycle\nSince Machine Learning (ML) is a collection of learning techniques used to extract value from data, we need to know what to say about data-at-hand, how machine learnt and how to evaluate learning model.\n\n\n\n\n\n\nFigure 1: ML Lifecycle1\n\n\n\n\n\n\n\n\nReferences\n\n1. Watson, A. Synthetic data and the data-centric machine learning life cycle. (2022).",
    "crumbs": [
      "Machine Learning Life Cycle"
    ]
  },
  {
    "objectID": "content/02_eda.html",
    "href": "content/02_eda.html",
    "title": "Overview",
    "section": "",
    "text": "Before venturing into any advanced analysis of data using statistical, machine learning, and algorithmic techniques, it is essential to perform basic data exploration to study the basic characteristics of a dataset.\nBy studying it basic properties, we may find useful patterns (trend), connections, and relationships within data. This is usually called data exploration or exploratory data analysis (EDA).\nThe whole idea is to get better understanding of the dataset at hand. We want to know, whether our data is good or not.\nOnce we have good data, we can training our machine learning model and get accurate results.\n\n\n\ndescriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot\n\n\n\n\n\n\n\n\nMany times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-analysis",
    "href": "content/02_eda.html#common-data-analysis",
    "title": "Overview",
    "section": "",
    "text": "descriptive statistic : summarizing, organizing, and presenting data meaningfully and concisely.\n\ncatagorize by group\ndata distribution\n\n\n\n\n\n\n\n\nFigure 1: Descriptive by visualization\n\n\n\n\ncorrelation analysis\n\nscatter plot \\(\\Longrightarrow\\) two variables are plotted along two axes.\npairplot \\(\\Longrightarrow\\) pairwise relationships between variables within a dataset\n\nThe closer the data points come to forming a straight line when plotted, the higher the correlation between the two variables, or the stronger the relationship.\nIf a relationship exists, the scatterplot indicates its direction and whether it is a linear or curved relationship. Relationships between variables can be described in many ways: positive or negative, strong or weak.\n\n\n\n\n\n\n\nFigure 2: correlation plot",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/02_eda.html#common-data-pre-processing",
    "href": "content/02_eda.html#common-data-pre-processing",
    "title": "Overview",
    "section": "",
    "text": "Many times in Machine Learning, we have to pre-process the data by “normalizing”, such as to set it to be in [0,1] (by scaling), or [-1,+1], scaling and centering, log or exponential, square root, etc. All these exercises do is only changing the scale and not the structure of the data. This is needed to assist computations and reduce computing errors, and not in any way doing anything beyond that. Furthermore, this will standardized numbers of various scales into same unique scale.\n\n\n\nImputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset. These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n\n\n\nTo reduce model biased toward certain patterns in data, data duplicate should be remove.",
    "crumbs": [
      "02 - EDA"
    ]
  },
  {
    "objectID": "content/04_learning.html",
    "href": "content/04_learning.html",
    "title": "About Learning I",
    "section": "",
    "text": "To automatically discover regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.\n\n\n\nThe basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).\n\n\n\nProblem statement must be made in probability statement(s).\nGiven a data, can we test whether our statement is : TRUE or FALSE.\ne.g. Given an image cheque whose size is 1264x616 pixel, is image Standard Charted cheque or not?\ne.g. Given a 3 bedroom apartment in Kelana Jaya, will the resale price be between RM1,400/sqf and below RM1500/sqf?\ne.g. Given 4 to 5 parameter of atmospheric reading in Petaling Jaya, what is the chance of raining the next 3 hours?\nAll these statements are probability statements, which answers will be TRUE or FALSE; but the answer will always be in terms of probability.\ne.g. The probability of a image cheque whose size is 1264x616 pixel is image Standard Charted cheque is 43%. The probability for the resale price of the said apartment in the range will be 91.3%. The probability of raining in Petaling Jaya for the coming 3 hours is 63.5%.\n\n\n\nFrom a statistical learning point-of-view:\n\\[\nY = f(\\bf{X}) + \\epsilon\n\\] where \\({X}\\) is input variable:\n\\[\nX = (X_1,X_2,...,X_p)\n\\] and \\(\\epsilon\\) is noise.\nThe task is to get prediction/estimation of:\n\\[\n\\hat{Y} = \\hat{f}(\\bf{X}) + \\hat{\\epsilon}\n\\] The task is then relegated to error estimator, by defining Loss function:\n\\[\nError(x) = E[(Y−\\hat{f}(\\bf{X}))^2]\n\\]\n\n\n\n\n\n\nFigure 1: Visualize learning model\n\n\n\nA loss function, also known as a cost function, is a method used to estimate the discrepancies between the actual and predicted values in a machine learning model. It provides a measure of how well the model is performing.\nnoise in data refers to unwanted modifications introduced to a source signal during the capture, storage, transmission, or processing of its information.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Example 1:Image data representation in vector\n\n\n\nBefore learning, data at hand must be representing in vector (e.g. Figure 2 & Figure 3)\n\n\n\n\n\n\nFigure 3: Example 2:Image data representation in vector\n\n\n\nBut wait, how about textual data ?\n\n\n\n\n\n\nFigure 4: Example 3:Text data representation in vector\n\n\n\n\n\n\nImagine you have a set of data. In order to have good estimated model, we have to split data at hand into:\n\nTraining set: This is the largest part in terms of the size of the dataset.\nValidation set: model training process is not a one-time process (highly iterative process). We have to train multiple models by trying different combinations of parameters (complexity). Then, we evaluate the performance of each model on the validation set.\nTest set: this is use after the training to evaluate performance of the model via un-seen data\n\n\n\n\n\n\n\nFigure 5: generic learning process\n\n\n\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 6: training loss plot\n\n\n\n\n\n\n\n\n\nFigure 7: Training trade-off2\n\n\n\n\n\n\n\n\n\nFigure 8: good plot\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization\n\n\n\n\n\nThe objective of “learning” is to simultaneously:\n\nAchieve LOW variance of Estimator\nAchieve LOW Bias of Estimator\n\n\n\n\n\n\n\n\n\nFigure 9: High Variance and High Bias\n\n\n\nWe have a poor estimator. Poor fit and poor predictor for training sample as well as for test sample.\n\n\n\n\n\n\n\n\n\nFigure 10: High Variance and Low Bias\n\n\n\nWe have a low “precision” predictor. In another word, we have an over-fitting, and hence the precision is poor.\nOverfitting: too much reliance on the training data\n\n\n\n\n\n\n\n\n\nFigure 11: Low Variance and High Bias\n\n\n\nWe have precise predictor, but will work well only for training sample, however will be problematic when applied to cases test sample. This is the case of underfitting.\nUnderfitting: a failure to learn the relationships in the data\n\n\n\n\n\n\n\n\n\nFigure 12: Low Variance and Low Bias\n\n\n\nThe predictor will have a good fit for both training sample and test sample. This is what we want.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#big-picture",
    "href": "content/04_learning.html#big-picture",
    "title": "About Learning I",
    "section": "",
    "text": "To automatically discover regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "href": "content/04_learning.html#the-problem-setting-of-statistical-learning.",
    "title": "About Learning I",
    "section": "",
    "text": "The basic setting of statistical learning is: given a problem statement, we want to find prediction model which estimate has best fit in providing solutions to the problem, using data at hand (in sample data) which has the Lowest Variance and Lowest Bias when applied to the data at hand as well as to the data not in hand (unseen data).",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#problem-statement",
    "href": "content/04_learning.html#problem-statement",
    "title": "About Learning I",
    "section": "",
    "text": "Problem statement must be made in probability statement(s).\nGiven a data, can we test whether our statement is : TRUE or FALSE.\ne.g. Given an image cheque whose size is 1264x616 pixel, is image Standard Charted cheque or not?\ne.g. Given a 3 bedroom apartment in Kelana Jaya, will the resale price be between RM1,400/sqf and below RM1500/sqf?\ne.g. Given 4 to 5 parameter of atmospheric reading in Petaling Jaya, what is the chance of raining the next 3 hours?\nAll these statements are probability statements, which answers will be TRUE or FALSE; but the answer will always be in terms of probability.\ne.g. The probability of a image cheque whose size is 1264x616 pixel is image Standard Charted cheque is 43%. The probability for the resale price of the said apartment in the range will be 91.3%. The probability of raining in Petaling Jaya for the coming 3 hours is 63.5%.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#statistical-modelling-hasni",
    "href": "content/04_learning.html#statistical-modelling-hasni",
    "title": "About Learning I",
    "section": "",
    "text": "From a statistical learning point-of-view:\n\\[\nY = f(\\bf{X}) + \\epsilon\n\\] where \\({X}\\) is input variable:\n\\[\nX = (X_1,X_2,...,X_p)\n\\] and \\(\\epsilon\\) is noise.\nThe task is to get prediction/estimation of:\n\\[\n\\hat{Y} = \\hat{f}(\\bf{X}) + \\hat{\\epsilon}\n\\] The task is then relegated to error estimator, by defining Loss function:\n\\[\nError(x) = E[(Y−\\hat{f}(\\bf{X}))^2]\n\\]\n\n\n\n\n\n\nFigure 1: Visualize learning model\n\n\n\nA loss function, also known as a cost function, is a method used to estimate the discrepancies between the actual and predicted values in a machine learning model. It provides a measure of how well the model is performing.\nnoise in data refers to unwanted modifications introduced to a source signal during the capture, storage, transmission, or processing of its information.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#how-to-learn",
    "href": "content/04_learning.html#how-to-learn",
    "title": "About Learning I",
    "section": "",
    "text": "Figure 2: Example 1:Image data representation in vector\n\n\n\nBefore learning, data at hand must be representing in vector (e.g. Figure 2 & Figure 3)\n\n\n\n\n\n\nFigure 3: Example 2:Image data representation in vector\n\n\n\nBut wait, how about textual data ?\n\n\n\n\n\n\nFigure 4: Example 3:Text data representation in vector\n\n\n\n\n\n\nImagine you have a set of data. In order to have good estimated model, we have to split data at hand into:\n\nTraining set: This is the largest part in terms of the size of the dataset.\nValidation set: model training process is not a one-time process (highly iterative process). We have to train multiple models by trying different combinations of parameters (complexity). Then, we evaluate the performance of each model on the validation set.\nTest set: this is use after the training to evaluate performance of the model via un-seen data\n\n\n\n\n\n\n\nFigure 5: generic learning process\n\n\n\n\n\nWhat do we want? \\(\\Longrightarrow\\) To make predictions on unseen data \\(\\Longrightarrow\\) We want a model that generalizes well \\(\\Longrightarrow\\) generalizes to unseen data\nHow we will do this? \\(\\Longrightarrow\\) controlling the complexity of the model (learning parameter)\nHow do we know if our model generalizes? \\(\\Longrightarrow\\) evaluating on test data.\n\n\n\n\n\n\n\nFigure 6: training loss plot\n\n\n\n\n\n\n\n\n\nFigure 7: Training trade-off2\n\n\n\n\n\n\n\n\n\nFigure 8: good plot\n\n\n\n\nLearning is NOT memorization! The ability to produce correct outputs on previously unseen inputs is called generalization",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/04_learning.html#error-and-bias",
    "href": "content/04_learning.html#error-and-bias",
    "title": "About Learning I",
    "section": "",
    "text": "The objective of “learning” is to simultaneously:\n\nAchieve LOW variance of Estimator\nAchieve LOW Bias of Estimator\n\n\n\n\n\n\n\n\n\nFigure 9: High Variance and High Bias\n\n\n\nWe have a poor estimator. Poor fit and poor predictor for training sample as well as for test sample.\n\n\n\n\n\n\n\n\n\nFigure 10: High Variance and Low Bias\n\n\n\nWe have a low “precision” predictor. In another word, we have an over-fitting, and hence the precision is poor.\nOverfitting: too much reliance on the training data\n\n\n\n\n\n\n\n\n\nFigure 11: Low Variance and High Bias\n\n\n\nWe have precise predictor, but will work well only for training sample, however will be problematic when applied to cases test sample. This is the case of underfitting.\nUnderfitting: a failure to learn the relationships in the data\n\n\n\n\n\n\n\n\n\nFigure 12: Low Variance and Low Bias\n\n\n\nThe predictor will have a good fit for both training sample and test sample. This is what we want.",
    "crumbs": [
      "04 - Model Learning"
    ]
  },
  {
    "objectID": "content/06_deployment.html",
    "href": "content/06_deployment.html",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "When a data scientist has a model ready, the next step is to deploy it in a way that it can serve the application.\nThe basic meaning of model serving is to host machine-learning models (on the cloud or on premises) and to make their functions available via API so that applications can incorporate AI into their systems.1\nDeploying a machine-learning model in production also involves resource management and model monitoring including operations stats as well as model drifts.\nThings to consider:\n\nAccess points (endpoints): An endpoint is a URL that allows applications to communicate with the target service via HTTPS protocol\nTraffic management: Requests at an endpoint go through various routes, depending on the destination service. Traffic management may also deploy a load-balancing feature to process requests concurrently.\nPre- and post-processing requests: A service may need to transform request messages into the format suitable for the target model and convert response messages into the format required by client applications. Often, serverless functions can handle such transformations.\nMonitor model drifts: We must monitor how each machine-learning model performs and detect when the performance deteriorates and requires retraining.\n\n\n\nServing the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!\n\n\n\nModel monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.\n\n\n\n\n\nA conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "06 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/06_deployment.html#model-serving-strategy",
    "href": "content/06_deployment.html#model-serving-strategy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Serving the model as:\n\nAnalytic system that make data-driven decisions\nOperational system to build data-powered products\n\nFor both method, things to consider is:\n\nwhether model embedded in the app or not\nwhether model served as an API\npre-trained model used as a library\n\nChallenge in model serving is always scalability while monitoring model drift!",
    "crumbs": [
      "06 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/06_deployment.html#model-monitor",
    "href": "content/06_deployment.html#model-monitor",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "Model monitoring is the ongoing process of tracking, analyzing, and evaluating the performance and behavior of machine learning models in real-world, production environments.2\n\n\n\ninput data: Models depend on the data received as input. If a model receives an input it does not expect, the model may break.\ndata quality: To maintain data integrity, you must validate production data before it sees the machine learning model, using metrics based on data properties. In other words, ensure that data types are equivalent.\ndata drift: Changes in distribution between the training data and production data can be monitored to check for drift: this is done by detecting changes in the statistical properties of feature values over time.",
    "crumbs": [
      "06 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/06_deployment.html#privacy",
    "href": "content/06_deployment.html#privacy",
    "title": "Model Serving & Montoring",
    "section": "",
    "text": "A conventional approach was to gather all data at a central server and use it to train the model. But this method, while easy, has raised concerns about data privacy, leaving a lot of valuable but sensitive data inaccessible.\nTo address this issue, AI models started to shift to a decentralized approach, and a new concept called “federated learning” has emerged.\nFederated learning is used for distributed training of machine learning algorithms on multiple edge devices without exchanging training data.\nEasy concept but challenging4 to implement due to:\n\nEfficient Communication across the federated network: communication in the network can be slower than local computation by many orders of magnitude.federated learning depends on communication-efficient methods that iteratively send small messages or model updates over the network\nManaging heterogeneous systems in the same networks: The storage, computational, and communication capabilities of the devices that are part of a federated network may differ significantly. Differences usually occur due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power supply (battery level).\nStatistical heterogeneity of data in federated networks: Devices frequently generate and collect data in a non-identically distributed manner across the network. Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data and in terms of analyzing the convergence behavior of associated training procedures\nPrivacy concerns and privacy-preserving methods:sharing other information such as model updates as part of the training process can also potentially reveal sensitive information, either to a third party or to the central server",
    "crumbs": [
      "06 - Model Deployment & Monitoring"
    ]
  },
  {
    "objectID": "content/use_case.html",
    "href": "content/use_case.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "So far we have learnt"
  },
  {
    "objectID": "content/use_case.html#model-trade-off",
    "href": "content/use_case.html#model-trade-off",
    "title": "Supervised Learning",
    "section": "Model Trade-Off",
    "text": "Model Trade-Off\n\n\n\n\n\n\nFigure 2: Trade-off between model interpretability and performance\n\n\n\nWhen building a model, things to consider:"
  },
  {
    "objectID": "content/use_case.html#use-case",
    "href": "content/use_case.html#use-case",
    "title": "Supervised Learning",
    "section": "Use Case",
    "text": "Use Case\n\nCase-1: Face Recognition/Verification\n\n\nCase-2: Weather(Rain) Prediction\n\n\nCase-3: Housing Price Prediction"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]