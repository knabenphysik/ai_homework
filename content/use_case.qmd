So far we have learnt

- ML life-cycle
- Data representation as vector
- basic of "learning"


# Supervised Learning

Supervised learning is a machine learning approach that is s defined by its use of **_labeled datasets_**. These datasets are designed to train or “supervise” algorithms into classifying data or predicting outcomes accurately. Using labeled inputs and outputs, the model can measure its accuracy and learn over time.

# Unsupervised Learning

Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled data sets. These algorithms **_discover hidden patterns_** in data without the need for human intervention 


![Type of Learning](image/concept_learning.png){#fig-ml-type}


## Model Trade-Off

![Trade-off between model interpretability and performance](image/model_trade_off.png){#fig-ml-tradeoff}

When building a model, things to consider:

- 
- 
- 

## Use Case

### Case-1: Face Recognition/Verification

### Case-2: Weather(Rain) Prediction

### Case-3: Housing Price Prediction

---

List of possible question (theory):


1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.

    (a) The sample size n is extremely large, and the number of predictors p is small.
    (b) The number of predictors p is extremely large, and the number of observations n is small.
    (c) The relationship between the predictors and response is highly non-linear.
    (d) The variance of the error terms, i.e. $\sigma^2$ = Var$(\epsilon)$, is extremely high.
    
    
2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

    (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO          salary. We are interested in understanding which factors affect CEO salary.
    
    (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar        products that were previously launched. For each product we have recorded whether it was a success or failure, price
    charged for the product, marketing budget, competition price,and ten other variables.
    
    (c) We are interested in predicting the % change in the USD/Euro
    exchange rate in relation to the weekly changes in the world
    stock markets. Hence we collect weekly data for all of 2012. For
    each week we record the % change in the USD/Euro, the %
    change in the US market, the % change in the British market,
    and the % change in the German market.
    
3. We now revisit the bias-variance decomposition.

    (a) Provide a sketch of typical (squared) bias, variance, training error,
    test error, and Bayes (or irreducible) error curves, on a single
    plot, as we go from less flexible statistical learning methods
    towards more flexible approaches. The x-axis should represent
    the amount of flexibility in the method, and the y-axis should
    represent the values for each curve. There should be five curves.
    Make sure to label each one.
    (b) Explain why each of the five curves has the shape displayed in
    part (a).
    
4. You will now think of some real-life applications for statistical learning.

    (a) Describe three real-life applications in which classification might
    be useful. Describe the response, as well as the predictors. Is the
    goal of each application inference or prediction? Explain your
    answer.
    (b) Describe three real-life applications in which regression might
    be useful. Describe the response, as well as the predictors. Is the
    goal of each application inference or prediction? Explain your
    answer.
    (c) Describe three real-life applications in which cluster analysis
    might be useful.
    
5. What are the advantages and disadvantages of a very flexible (versus
a less flexible) approach for regression or classification? Under what
circumstances might a more flexible approach be preferred to a less
flexible approach? When might a less flexible approach be preferred?


6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric
approach to regression or classification (as opposed to a nonparametric
approach)? What are its disadvantages?

7. Suppose that we use some statistical learning method to make a prediction
for the response Y for a particular value of the predictor X.
Carefully describe how we might estimate the standard deviation of
our prediction.

8. What is a feature space? What do the dimensions of such a space represent?
What is a vector? What is a feature vector?

9.If we want to use the values of F different features, in order to classify objects,
where each feature can have any of G different values, what is the dimension
of the feature space?

10. For a 12 × 12 grayscale image (256 grayscale levels), how many dimensions
are there for the feature vector? How many different possible feature vectors
are there?

11. True or false: A probability density can take on values greater than 1.

12. True or false: A probability density can take on values less than 0.

13. What is a decision rule? If there are 10 possible feature vectors, how many
possible decision rules are there?

14. What are the advantages of choosing/using more features and what are the
disadvantages?

1. What is the general learning problem we are concerned with?
2. Why do we need learning? Why cannot we simply use a Bayes rule for pattern
classification?
3. What are training data for learning from examples?
4. What sorts of assumptions do we need to make about the training data?
7. What is the curse of dimensionality?
8. What is inductive bias? Is it good or bad?

1. a(a) Sketch a diagram of a perceptron with three inputs x1, x2, x3 and weights
w1,w2,w3. Label the inputs, weights, and output.

2 consider a classification problem in which each
instance consists of $d$ features $x1, . . . , xd$, each of which can take on only the
values 0 or 1. A feature vector belongs to class 0 if $x_1 + x_2 +· · ·+x_d$ is even
(i.e., the number of 1’s is even) and it belongs to class 1 otherwise. Can this
problem be solved by a single perceptron? A three-layered network? Why or
why not?

3. If a loss function other than squared error is used, will the regression function
(i.e., the conditional mean of y, given $\bar{x}$) still always be the best estimator?

4. with less than 10 words, what is the purpose of machine learning?

5. Imagine you are solving a classification problem with a highly imbalanced class.

      The majority class is observed 99% of the time in the training data. Your model has 99% accuracy after taking the predictions on the test set. Which of the following is true in such a case?
      
          1.The accuracy metric is not a good idea for imbalanced class problems.
          2.The accuracy metric is a good idea for imbalanced class problems.
          3.Precision and recall metrics are good for imbalanced class problems.
          4.Precision and recall metrics aren’t good for imbalanced class problems.
      
      A) 1 and 3
      B) 1 and 4
      C) 2 and 3
      D) 2 and 4
      
      Solution: (A)
      
6. In machine learning, what is the primary difference between supervised and unsupervised learning?

    1.Supervised learning involves data that has been labeled and classified, while unsupervised learning data is unlabeled and unclassified.
    2.Supervised learning is monitored closely by data scientists, while they don't play a role in unsupervised learning.
    3.Supervised learning is only used for image recognition, while unsupervised learning can be used for various analytics applications.
    
7. True or false? Bias is a common problem in data science applications.

8. Why is data sampling useful for data scientists?

    1. It lets them analyze data sets in small batches to reduce their use of system resources.
    2. It reduces the amount of data storage space that's required for data science applications.
    3. It enables them to use a representative subset of data to build accurate analytical models more quickly.
    
9. In traditional computer programming, you input commands. What do you input with machine learning?
    1. patterns
    2. programs
    3. rules
    4. data

12. Why is it important for machine learning algorithms to have access to high-quality data?

    It will take too long for programmers to scrub poor data.
    If the data is high quality, the algorithms will be easier to develop.
    Low-quality data requires much more processing power than high-quality data.
    If the data is low quality, you will get inaccurate results.[aws]


    
10. Your company wants to predict whether existing automotive insurance customers are more likely to buy homeowners insurance. It created a model to better predict the best customers contact about homeowners insurance, and the model had a low variance but high bias. What does that say about the data model?

    1.It was consistently wrong.(ini jwABAN)
    2.It was inconsistently wrong.
    3. It was consistently right.
    4. It was equally right end wrong.
    
11. You want to identify global weather patterns that may have been affected by climate change. To do so, you want to use machine learning algorithms to find patterns that would otherwise be imperceptible to a human meteorologist. What is the place to start?

    1. Find labeled data of sunny days so that the machine will learn to identify bad weather.
    2. Use unsupervised learning have the machine look for anomalies in a massive weather database.
    3. Create a training set of unusual patterns and ask the machine learning algorithms to classify them.
    4. Create a training set of normal weather and have the machine look for similar patterns. [anw]
    
12. You work for a power company that owns hundreds of thousands of electric meters. These meters are connected to the internet and transmit energy usage data in real-time. Your supervisor asks you to direct project to use machine learning to analyze this usage data. Why are machine learning algorithms ideal in this scenario?

    The algorithms would help the meters access the internet.
    The algorithms will improve the wireless connectivity.
    The algorithms would help your organization see patterns of the data.[anw]
    By using machine learning algorithms, you are creating an IoT device.

13. How is machine learning related to artificial intelligence?

    Artificial intelligence focuses on classification, while machine learning is about clustering data.
    Machine learning is a type of artificial intelligence that relies on learning through data.
    Artificial intelligence is form of unsupervised machine learning.
    Machine learning and artificial intelligence are the same thing.
    
14. What is one reason not to use the same data for both your training set and your testing set?

    You will almost certainly underfit the model.
    You will pick the wrong algorithm.
    You might not have enough data for both.
    You will almost certainly overfit the model.[anw]
    
15. Are data model bias and variance a challenge with unsupervised learning?

    No, data model bias and variance are only a challenge with reinforcement learning.
    Yes, data model bias is a challenge when the machine creates clusters.[anw]
    Yes, data model variance trains the unsupervised machine learning algorithm.
    No, data model bias and variance involve supervised learning.


16. Many of the advances in machine learning have come from improved ___.

    statistics
    structured data
    availability
    algorithms[anw]


17. You work for a website that enables customers see all images of themselves on the internet by uploading one self-photo. Your data model uses 5 characteristics to match people to their foto: color, eye, gender, eyeglasses and facial hair. Your customers have been complaining that get tens of thousands of photos without them. What is the problem?

    You are overfitting the model to the data
    You need a smaller training set
    You are underfitting the model to the data[anw]
    You need a larger training set
    
18. ___ refers to a model that can neither model the training data nor generalize to new data.

    good fitting
    overfitting
    underfitting[anw]
    all of the above

19. What does it mean to underfit your data model?

    There is too little data in your training set.
    There is too much data in your training set.
    There is not a lot of variance but there is a high bias.[anw]
    Your model has low bias but high variance.
    
    Explanation: Underfitted data models usually have high bias and low variance. Overfitted data models have low bias and high variance.
    
20. Asian user complains that your company's facial recognition model does not properly identify their facial expressions. What should you do?

    Include Asian faces in your test data and retrain your model.
    Retrain your model with updated hyperparameter values.
    Retrain your model with smaller batch sizes.
    Include Asian faces in your training data and retrain your model.[anw]
    
21. The new dataset you have just scraped seems to exhibit lots of missing values. What action will help you minimizing that problem?

    Wise fill-in of controlled random values
    Replace missing values with averaging across all samples
    Remove defective samples
    Imputation[anw]
    
22. Which choice is the best example of labeled data?

    a spreadsheet [anw]
    20,000 recorded voicemail messages
    100,000 images of automobiles
    hundreds of gigabytes of audio files

23. The data in your model has low bias and low variance. How would you expect the data points to be grouped together on the diagram?

    They would be grouped tightly together in the predicted outcome.[anw]
    They would be grouped tightly together but far from the predicted.
    They would be scattered around the predict outcome.
    They would be scattered far away from the predicted outcome.

24. In supervised machine learning, data scientist often have the challenge of balancing between underfitting or overfitting their data model. They often have to adjust the training set to make better predictions. What is this balance called?

    the under/over challenge
    balance between clustering classification
    bias-variance trade-off[anw]
    the multiclass training set challenge

25. If you are thinking about using machine learning algorithms, the best thing you can do today is to ensure you have quality _.

    data[anw]
    processors
    networking
    statistical techniques

26. You've received 1,000,000 images and have split it in 96%/2%/2% between train, dev and test sets. You've trained your model, and analyzed the results. After working further on the problem, you’ve decided to correct the incorrectly labeled data on the dev set.

Which of these statements do you agree with?

    You should also correct the incorrectly labeled data in the test set, so that the dev and test sets still come from the same distribution.[anw]
    You should correct incorrectly labeled data in the training set as well so as to avoid your training set now being even more different from your dev set.
    You should not correct the incorrectly labeled data in the test set, because the test set should reflect the data distribution of the real world.
    If you want to correct incorrectly labeled data, you should do it on all three sets (train/dev/test) in order to maintain similar distributions.

27. In simple words, Define precision and recall.

28. As the number of training examples goes to infinity, your model trained on that data will have:

        A. Lower variance [anw]
        B. Higher variance 
        C. Same variance
        
  Explanation: It is important that your dev and test set have the closest possible distribution to "real" data.

29. During the data preprocessing step, how should one treat missing/null values? How will you deal with them?

20. Which approach is used for handling imbalanced datasets in classification tasks, where one class has significantly fewer samples than the others?

    a) Overfitting
    b) Data Augmentation
    c) Oversampling the minority class
    d) Feature Scaling
    
    Answer: c) Oversampling the minority class

